{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de LM.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"VmlPt7GSPg7g","colab_type":"text"},"cell_type":"markdown","source":["# Modelado de lenguaje --> Language Modeling\n","\n","## ¿Qué es?\n","\n","En esta parte veremos una de las tareas más complejas de NLP. La generación de texto. Es decir, si queremos tener un asistente virtual, pero no queremos programarle las respuestas, ¿Qué opciones tenemos? Una de ellas es Language Modeling. De todas formas esto último aún no ha funcionado bien nunca, así que vamos a lo que sí ha funcionado? El algoritmo que predice o os sugiere la siguiente palabra en vuestro teclado? Language Modeling. ¿Os acordáis cuándo aplicamos la distancia de edición en palabras? ¿Que pasa si queremos aplicar distancias de edición pero las sugerencias de la edición estan a la misma distáncia? Podríamos aplicar Language Modeling y ver cuál es la mejor opción. \n","\n","Técnicamente, el modelado de lenguaje es una distribución de probabilidades sobre secuencias de palabras. Que significa esto?\n","\n","![](https://i.imgur.com/OWHNbj9.jpg)\n","\n","![](https://i.imgur.com/V9jXP6J.jpg)\n","\n","## Como lo hacemos?\n","Es decir, es una manera de asignar una probabilidad a auna frase o texto. Como lo hacemos? Con Bayes. Tenemos los conteos hechos de bayes, solo hay que aplicar la asumpción de independéncia.\n","\n","### Dependencia \n","![](https://i.imgur.com/3ARmOZr.jpg)\n","\n","### Independencia\n","![](https://i.imgur.com/JBn2GhN.jpg)\n","\n","#### bayes! \n","### BAYES! \n","## BAYES!\n","\n","Contamos! --> asignamos probabilidades a que ocurran los n-grams que decidamos usar... y ya casi lo tenemos listo para su uso!\n","\n","\n","### Ejemplos de uso:\n","\n","Ejemplo:\n","\n","![](https://i.imgur.com/IlxTT9P.jpg)\n","\n","Otro ejemplo.\n","\n","![](https://i.imgur.com/T2ZUr6y.jpg)"]},{"metadata":{"id":"oCIZs38k0hpX","colab_type":"text"},"cell_type":"markdown","source":["## Imports\n"]},{"metadata":{"id":"722HooI4Guug","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install spacy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H81O4LGjHYxH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":235},"outputId":"e1f31046-8f50-48e8-d7f7-528d64e621c1","executionInfo":{"status":"ok","timestamp":1530437732405,"user_tz":-120,"elapsed":10582,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n","\u001b[K    100% |████████████████████████████████| 37.4MB 45.5MB/s \n","\u001b[?25hInstalling collected packages: en-core-web-sm\n","  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_sm\n","\n","    You can now load the model via spacy.load('en_core_web_sm')\n","\n"],"name":"stdout"}]},{"metadata":{"id":"6G7BAc_kGn2g","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import spacy\n","\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dke06fuTIdM2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["doc = nlp('this is a test of a text processed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qAlDaqL4JEYL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":151},"outputId":"4bdfb6a3-540b-4205-cd5f-7dfec5b6e7ec","executionInfo":{"status":"ok","timestamp":1530437735942,"user_tz":-120,"elapsed":466,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["[(t.text, t.pos_, t.dep_) for t in doc]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('this', 'DET', 'nsubj'),\n"," ('is', 'VERB', 'ROOT'),\n"," ('a', 'DET', 'det'),\n"," ('test', 'NOUN', 'attr'),\n"," ('of', 'ADP', 'prep'),\n"," ('a', 'DET', 'det'),\n"," ('text', 'NOUN', 'pobj'),\n"," ('processed', 'VERB', 'acl')]"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"7hrkOFNL0rrG","colab_type":"text"},"cell_type":"markdown","source":["## Preprocesado de texto"]},{"metadata":{"id":"vHFBXGCGKL_0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"hObW4G5USdRY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["corpus =[\n","    'this is an apple', 'this is a document', 'we are in a Keepcoding course',\n","    'we are teaching a language model', 'the corpus will be super small',\n","    'this is also a test', 'we will see what we can do', 'this is an orange',\n","    'apples are not oranges', 'apples and oranges are fruits',\n","    'fruits are not vegatables', 'this is Sparta', 'is an orange a fruit'\n","        ]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"diDD1Dx_hFj1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"oFMu4tzT04LW","colab_type":"text"},"cell_type":"markdown","source":["## Preparación del modelo"]},{"metadata":{"id":"JgWoLZdAOhbR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"QMpu0kZMTD1O","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"collapsed":true,"outputId":"6e5cc0ae-f4ee-4964-866f-5f83554a5b7f","executionInfo":{"status":"ok","timestamp":1530437740917,"user_tz":-120,"elapsed":447,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('this', 5), ('is', 6), ('an', 3), ('apple', 1), ('a', 5)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"hGiQ2WMXTvlT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"collapsed":true,"outputId":"7a863258-e310-4176-e864-c757f1eb4b62","executionInfo":{"status":"ok","timestamp":1530437741942,"user_tz":-120,"elapsed":449,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('this is', 5), ('is an', 3), ('an apple', 1), ('is a', 1), ('a document', 1)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"sND5D25qMYhz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"bFIQxMmoXtRC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"P0pYUMXhdp44","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"hEmVUCzlqixY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"wuMu_CKCZpPI","colab_type":"text"},"cell_type":"markdown","source":["Compute the probability of a sentence? Yes We Can!\n","\n","Pero lo haremos en log space! Nada de multiplicar, porque cuando estemos trabajando con probabilidades muy pequenas y encima las estemos multiplicando, tendremos un problema de \"underflow\"."]},{"metadata":{"id":"D5vuQTjAZm9o","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"DqLLxxpfbWlX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xMLwWqWnbvlD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"4MDskhbGcOJc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yen_AzGhrMlO","colab_type":"text"},"cell_type":"markdown","source":["¿Qué acaba de pasar?\n","\n","Ver P(x) = 0\n","\n","## Smoothing - Laplace\n","\n","En todos estos modelos, siempre hay que tener en cuenta, que pasa con todas esas palabras/tokens que no hemos visto en el entreno.\n","\n","Que podemos hacer? Una solución muy simple se llama Laplace smoothing o add-one smoothing. Consiste en añadir +1 a todos los conteos, y así, hacemos ver que todas esas palabras que no hemos visto nunca, por lo menos han sido \"vistas\" una vez en nuestro train set.\n","\n","No es ni mucho menos la mejor de las opciones, el más usado es [Kneser-Ney](http://www.foldl.me/2014/kneser-ney-smoothing/), os dejo el link aquí. No lo implementaremos, pero este tutorial contiene información muy valiosa, e intuiciones muy válidas."]},{"metadata":{"id":"SP_WDwDR-Oga","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7dwVlSBsJRqP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"SG_JnZSp_Tke","colab_type":"text"},"cell_type":"markdown","source":["# ¿Cómo evaluamos un sistema de LM?\n","\n","Normalmente usaríamos medidas cómo las que ya habéis visto: accuracy, precision, recall, F-1... En LM, no usamos ninguna de ellas. Usamos una medida que se llama perplexity. Perplejidad.\n","\n","Una analogía muy buena que he leído últimamente es la siguiente. Cuándo nace un bebé, la perplejidad es muy alta, porque no tenemos ni idea de lo que va a decir (es un modelo sin entrenar). Cuando el bebé empieza a crecer, y a aprender sus primeras palabras y frases, la perplejidad va disminuyendo poco a poco, porque cada vez es más predecible saber que va a decir. Hasta que llega un momento, que os sorprende muy poco la estructura de sus frases porque ya ha aprendido un modelo de lenguaje muy particular. Este modelo no tiene que ser un castellano perfecto, o un inglés perfecto, es el lenguaje que habla en su día a día, en su casa, en el colegio, o donde sea que aprenda, justo igual que nuestros modelos. Si aprenden de un lenguaje muy coloquial, no podemos esperar que generen frases muy formales y viceversa. Pues esto es la perplejidad.\n","\n","No entraremos en detalles de cómo se llego aquí, pero para que os hagáis una idea, viene de Claude Shannon y su teoría de la información. Lo que viene a decirnos es, como podemos guardar el máximo de información en los menores bits, es decir, quan bien somos capaces de predecir la siguiente palabra, y optimizar un modelo acorde a ello.\n","\n","La fórmula de perplexity que implementaremos es la siguiente:\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/0WwyBbc.png)\n","</div>\n","\n","\n"]},{"metadata":{"id":"yHl0Jrxc_R-S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"1WnyaxU5Nigq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"anmzAnx4f4dx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cb1w2QFTPYU8","colab_type":"text"},"cell_type":"markdown","source":["## Generar frases!"]},{"metadata":{"id":"F_w4MTcZqV5A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"YPwuw6en2Qok","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"OmyRDoyxAaEs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}