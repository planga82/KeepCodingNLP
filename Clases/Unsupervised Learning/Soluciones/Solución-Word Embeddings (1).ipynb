{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Solución-Word Embeddings.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["hRt_Y3_UpPon","q2ypGdYWpMFo","WHvr2ZDgpU_z","D_Pg29vHpYg7","QpEpFY5Kpc71","ElRtLX60pftQ","ABtEOOnNpi_8"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wCLgSWjzNeQP","colab_type":"text"},"cell_type":"markdown","source":["# Intro to class\n","\n","\n","[word embeddings colah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)"]},{"metadata":{"id":"NoWTG-caNeQZ","colab_type":"text"},"cell_type":"markdown","source":["# Word Embeddings\n","\n","Cuando empezó esto de los word embeddings? Hace algo de tiempo, aquí por ejemplo. [Distributed Representations](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2841&context=compsci)\n","\n","![Word Embeddings](https://qph.fs.quoracdn.net/main-qimg-e8b83b14d7261d75754a92d0d3605e36 \"Word Embeddings\")\n","\n","Los word embeddings son vectores densos que representan tokens, aunque el término es word, sería más correcto usar token embeddings.\n","\n","Hay varias implementaciones, como CBOW o Skip-Gram (del famoso word2vec) o Global Vectors (GloVe). \n","\n","Són una solución al problema de dimensionalidad que comentamos el primer día de clase.\n","\n","### Vale pero que son realmente los valores de los word embeddings?\n","\n","Los valores de los word embeddings representan \"clases\", aunque es muy complicado estar 100% seguro de que clases son, sobretodo porque cada vez que entrenemos word embeddings, el modelo puede aprender unas u otras clases. Con lo cuál, hay que olvidar-se un poco del concepto de vector dónde la posición 1 siempre tendremos la clase 1, y en la posición 2 la clase 2.\n","\n","![](https://cdn-images-1.medium.com/max/1600/1*mLrheV1nGz7XemDAVRcZ4A.png)\n","\n","Pensad que cuando escogemos la dimensionalidad de estos, ya estamos fijando cuántas clases puede aprender, y además, cada dimensionalidad pueden ser una mezcla de clases. Así pues, y el objetivo de aprender word vectors, es el siguiente:\n","\n","> *Rezamos para que palabras \"similares\" tengan vectores similares*. Es decir a representar palabras por su semantismo.\n","\n","### Recordatorio:"]},{"metadata":{"id":"MuV18XhINeQf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"0cc46864-732a-4971-c955-d2c4ab814467","executionInfo":{"status":"ok","timestamp":1530776671727,"user_tz":-120,"elapsed":615,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["import numpy as np\n","import io"],"execution_count":2,"outputs":[]},{"metadata":{"id":"ogoLmh-4NeQj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":54},"outputId":"b1935ed5-fa52-44bf-dfc9-df3d40977f1c","executionInfo":{"status":"ok","timestamp":1530776611052,"user_tz":-120,"elapsed":432,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["# Bag-of-Words\n","nb_features = 10\n","bow_doc = np.zeros(shape=(1,nb_features))\n","print(bow_doc.shape)\n","nb_docs = 5\n","bow_docs = np.zeros(shape=(nb_docs, nb_features))\n","print(bow_docs.shape)\n","# Si quisieramos representar cada posición del documento, con una feature, simplemente habría \n","# que generar un vector de maxlen features.\n","doc_representation = np.array([1, 2, 3, 4, 5, 0 , 0]) # 0 padding"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(1, 10)\n","(5, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"dsAfLvSrNeQn","colab_type":"text"},"cell_type":"markdown","source":["Estas representaciones son simples features, pero podemos mejorar de alguna forma? Si quisieramos representar un documento con la ultima representación y ngrams por ejemplo, creariamos un array tal que asi:"]},{"metadata":{"id":"_qestvdJNeQo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"9798a1d9-4518-4115-82be-c2c0a30164f7","executionInfo":{"status":"ok","timestamp":1530776612666,"user_tz":-120,"elapsed":604,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["maxlen = len([1, 2, 3, 4, 5, 0 , 0])\n","doc_ngrams = np.zeros(shape=(maxlen, nb_features))\n","doc_ngrams.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7, 10)"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"R0TI-XgFNeQu","colab_type":"text"},"cell_type":"markdown","source":["## Problemas que arreglamos\n","\n","Los shapes de las representaciones son muy indicativas del problema que queremos resolver. Es decir, si ahora en lugar de una frase, queremos representar un documento de 400 posiciones, y que tiene 20k features? Por cada documento generaremos arrays de shape=(400, 20000), lo cual se hace insostenible a medida de que hagamos crecer las features. \n","\n","Ahora que ya habeis visto la cantidad de datos necesarios para ciertos problemas como computer vision, texto no ha sido indiferente a la revolución del \"big data\". Aún así, los word embeddings NO son deep learning, pero si se usan en él. Sás las features más usadas para texto en deep learning.\n","\n","### Semántica\n","\n","Además muchos problemas de NLP se basan basicamente en la semantica de estos tokens, y hasta la fecha no hemos visitado demasiado la semantica de las palabras. Usando Word Embeddings podremos boostear nuestras features y consecuentemente nuestros outputs de los algoritmos. \n","\n","Haremos uns dummy implementación del CBOW y GloVe, pero podéis encontrar vectores ya entrenados, o algoritmos implementados de forma eficiente en librerías como gensim o spaCy. \n","\n","CBOW esta basado en redes neuronales, y GloVe en factorización de matríces.\n","\n","<div align=\"center\">\n","    ![cbow.png](https://adriancolyer.files.wordpress.com/2016/04/word2vec-cbow.png =500x)\n","</div>"]},{"metadata":{"id":"hRt_Y3_UpPon","colab_type":"text"},"cell_type":"markdown","source":["### Load Data"]},{"metadata":{"id":"esd2U8EbNeQy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":50},"outputId":"a5bcf026-b809-4a9c-9303-85ccf3c60e06","executionInfo":{"status":"ok","timestamp":1530815284247,"user_tz":-120,"elapsed":9351,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["!pip install tqdm\n","from tqdm import tqdm\n","\n","!pip install -q pydot\n","\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.4)\r\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"imEOahUTNeQ2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install spacy\n","!python -m spacy download es_core_news_sm\n","\n","import spacy\n","\n","nlp = spacy.load('es_core_news_sm', disable= ['ner', 'parser'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iMKO-xPcYRAH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":91},"outputId":"53d3530d-9188-4e71-d745-f43c0b88648b","executionInfo":{"status":"ok","timestamp":1530776699551,"user_tz":-120,"elapsed":23974,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-fd2f5b79-582b-4718-84a2-906639808096\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-fd2f5b79-582b-4718-84a2-906639808096\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving quijote.txt to quijote (1).txt\n","User uploaded file \"quijote.txt\" with length 2141457 bytes\n"],"name":"stdout"}]},{"metadata":{"id":"oBfkFoD4NeQ4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":17},"outputId":"44c3cc5d-7832-4e07-ceb8-e90b7b00268d","executionInfo":{"status":"ok","timestamp":1530777693635,"user_tz":-120,"elapsed":446,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["# construir el corpus\n","quijote = uploaded['quijote.txt']\n","quijote = io.StringIO(quijote.decode('utf-8')).getvalue()\n","\n","quijote = quijote.split('.')\n"],"execution_count":11,"outputs":[]},{"metadata":{"id":"q2ypGdYWpMFo","colab_type":"text"},"cell_type":"markdown","source":["### Dataset Preparation"]},{"metadata":{"id":"Yo0ZGqBQdgqG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"59f5342e-5c01-4e15-9ffa-8e49e5fed0e8","executionInfo":{"status":"ok","timestamp":1530777708447,"user_tz":-120,"elapsed":525,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["len(quijote)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8210"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"qYgEcEEtZOFE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":435},"outputId":"3607f665-f220-43fa-a6c5-848423449e67","executionInfo":{"status":"ok","timestamp":1530777776226,"user_tz":-120,"elapsed":33618,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["quijote_docs = []\n","for doc in tqdm(nlp.pipe(quijote, batch_size=1000, n_threads=4)):\n","    quijote_docs.append(doc)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","\n","0it [00:00, ?it/s]\u001b[A\u001b[A\n","\n","1it [00:04,  4.64s/it]\u001b[A\u001b[A\n","\n","1001it [00:08, 122.66it/s]\u001b[A\u001b[A\n","\n","2001it [00:12, 164.90it/s]\u001b[A\u001b[A\n","\n","3001it [00:16, 180.64it/s]\u001b[A\u001b[A\n","\n","4001it [00:20, 194.90it/s]\u001b[A\u001b[A\n","\n","5001it [00:24, 203.89it/s]\u001b[A\u001b[A\n","\n","6001it [00:28, 211.70it/s]\u001b[A\u001b[A\n","\n","7001it [00:32, 217.96it/s]\u001b[A\u001b[A\n","\n","8001it [00:32, 243.00it/s]\u001b[A\u001b[A\n","\n","8210it [00:32, 249.22it/s]\u001b[A\u001b[A"],"name":"stderr"}]},{"metadata":{"id":"QgrfPFjBNeQ9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"6616eb65-5430-4862-9b58-81b8ee6115a7","executionInfo":{"status":"ok","timestamp":1530778759952,"user_tz":-120,"elapsed":1003,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["vocab = list({t.text for doc in quijote_docs for t in doc})\n","vocab.insert(0, '<PAD>')"],"execution_count":15,"outputs":[]},{"metadata":{"id":"ZTVUzlwlNeRB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"6c2b9e2c-6b4f-4de7-a037-75ba85306c3a","executionInfo":{"status":"ok","timestamp":1530778760683,"user_tz":-120,"elapsed":462,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["w2id = {w:i for i, w in enumerate(vocab)}\n","id2w = {i:w for w, i in w2id.items()}"],"execution_count":16,"outputs":[]},{"metadata":{"id":"ERQitSr6NeRD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"0c15898a-e329-45e5-ff00-fb5ccea30e66","executionInfo":{"status":"ok","timestamp":1530778761296,"user_tz":-120,"elapsed":456,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["id2w[1]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'dividiéndolas'"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"k6kqwK5eNeRH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"dc179e74-70e0-47f6-986c-17bcbeb3ea16","executionInfo":{"status":"ok","timestamp":1530825429913,"user_tz":-120,"elapsed":466,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["config = {\n","    'vocab_size': 25166,\n","    'embed_size': 50,\n","    'context_size':2\n","}\n","config"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context_size': 2, 'embed_size': 50, 'vocab_size': 25166}"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"WHvr2ZDgpU_z","colab_type":"text"},"cell_type":"markdown","source":["### Generate Dataset"]},{"metadata":{"id":"_He3tIjjNeRL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"23943600-8b88-45df-c9af-e7202c425a2c","executionInfo":{"status":"ok","timestamp":1530778764783,"user_tz":-120,"elapsed":457,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["def generate_context_target(data, context_size, w2id, vocab_size):\n","    # x-2 x-1 y x+1 x+2\n","    for i in range(context_size, len(data)-context_size):\n","        x = np.zeros(shape=(context_size*2))\n","        y = np.zeros(shape=(vocab_size))\n","        y[w2id[data[i]]]=1\n","        for idx, j in enumerate(range(i-context_size, i+context_size)):\n","            if j < i:\n","                x[idx] = w2id[data[j]]\n","            else:\n","                x[idx] = w2id[data[j+1]]\n","        yield x.reshape(1,-1), y.reshape(1,-1)"],"execution_count":19,"outputs":[]},{"metadata":{"id":"QVdRe_LNNeRO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"38df5715-456f-441e-cc61-e94e0e9598e5","executionInfo":{"status":"ok","timestamp":1530778768487,"user_tz":-120,"elapsed":1119,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["quijote_corpus = [t.text for doc in quijote_docs for t in doc]\n","print(len(quijote_corpus))\n","# generator for x, y in generate_context_target(quijote_corpus, config['context_size'], w2id, config['vocab_size']):\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["467707\n"],"name":"stdout"}]},{"metadata":{"id":"D_Pg29vHpYg7","colab_type":"text"},"cell_type":"markdown","source":["### Create Model"]},{"metadata":{"id":"W5eNKTy7NeRU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qHo-PIwGNeRY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":235},"outputId":"32629c0c-aa3b-47b9-ad6a-5036a48cd21f","executionInfo":{"status":"ok","timestamp":1530825433109,"user_tz":-120,"elapsed":454,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["# build CBOW architecture\n","cbow = Sequential()\n","cbow.add(Embedding(input_dim=config['vocab_size'], output_dim=config['embed_size'], input_length=config['context_size']*2))\n","cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(config['embed_size'],)))\n","cbow.add(Dense(config['vocab_size'], activation='softmax'))\n","cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","cbow.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 4, 50)             1258300   \n","_________________________________________________________________\n","lambda_1 (Lambda)            (None, 50)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 25166)             1283466   \n","=================================================================\n","Total params: 2,541,766\n","Trainable params: 2,541,766\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"IyRST4w1NeRi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1038},"outputId":"faa57740-c27b-4677-d45e-d134ea8b3f4a","executionInfo":{"status":"error","timestamp":1530825435768,"user_tz":-120,"elapsed":646,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["\n","SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n","                 rankdir='TB').create(prog='dot', format='svg'))"],"execution_count":9,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1862\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1866\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1867\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a56bc77625d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n\u001b[0;32m----> 3\u001b[0;31m                  rankdir='TB').create(prog='dot', format='svg'))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         raise OSError(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n","\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."]}]},{"metadata":{"id":"QpEpFY5Kpc71","colab_type":"text"},"cell_type":"markdown","source":["### Train"]},{"metadata":{"id":"sM53ri2QNeRm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["if False:\n","    for epoch in range(1, 6):\n","        loss = 0.\n","        i = 0\n","        for x, y in generate_context_target(quijote_corpus, config['context_size'], w2id, config['vocab_size']):\n","            i += 1\n","            loss += cbow.train_on_batch(x, y)\n","            if i % 10000 == 0:\n","                print('Processed {} (context, word) pairs'.format(i))\n","\n","        print('Epoch:', epoch, '\\tLoss:', loss)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ElRtLX60pftQ","colab_type":"text"},"cell_type":"markdown","source":["### Get word embeddings"]},{"metadata":{"id":"jAQGZYVmNeRn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["word_embeddings = cbow.get_weights()[0]\n","word_embeddings = word_embeddings[1:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tYnn6QQ_NeRq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.metrics.pairwise import euclidean_distances"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ABtEOOnNpi_8","colab_type":"text"},"cell_type":"markdown","source":["### Test"]},{"metadata":{"id":"pcwLb7G7NeRs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# compute pairwise distance matrix\n","if False:\n","    distance_matrix = euclidean_distances(word_embeddings)\n","    print(distance_matrix.shape)\n","\n","    # view contextually similar words\n","    similar_words = {search_term: [id2w[idx] for idx in distance_matrix[w2id[search_term]-1].argsort()[1:6]+1] \n","                       for search_term in ['Quijote', 'Mancha', 'escudero', 'Sancho']}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O3b0utmpNeRz","colab_type":"text"},"cell_type":"markdown","source":["## GloVe\n","\n","El algoritmo que veremos se basa en la siguiente idea:\n","\n","<div align=\"center\">\n","![](https://image.slidesharecdn.com/analyticssummitnov082013final-131114141831-phpapp01/95/analytics-summit-2013-25-638.jpg?cb=1384439169 =300x)\n","</div>\n","\n","Es una idea que ya hemos visto presentada en formatos diferentes. Cuando entrenamos el Language Modeling, al final, estabamos asignando mayores probabilidades a palabras o tokens que aparecían en un contexto concreto, sólo que en ese caso, el contexto era sólo del pasado.\n","\n","Vamos a ver una implementación \"sencilla\" de GloVe. Como hemos comentado, está basada en dos cosas, la primera, en la factorización de matrices. No entraremos en detalles de que es la factorización de matrices, nos basta con saber que en el caso de GloVe, nuestro objetivo será, dado una matriz de co-ocurrencias, obtener dos matrices que multiplicadas reproduzcan la original.\n","\n","![glove.png](https://cdn-images-1.medium.com/max/800/1*UNtsSilztKXjLG99VXxSQw.png)\n","\n","La matriz lila, será nuestra matriz de co-ocurrencias. ¿Qué es una matriz de coocurrencias? Tan fácil como ir sumando, dado un contexto, esas palabras que van aparenciendo entre si. Aquí tenéis un ejemplo de su uso.\n","\n","![](https://cdn-images-1.medium.com/max/1600/0*Yl7I7bH52zk8m_8R.)\n","\n","Primera Cosa en la que tenemos que fijarnos. Que valor tienen words y context? Es el mismo valor?\n","\n","Y si es así, que hacemos con las dos matrices que generamos?\n","\n","#### Algoritmo\n","\n","![](https://i.imgur.com/HCo2ZwE.png)\n"]},{"metadata":{"id":"mX5hgPwMFaSY","colab_type":"text"},"cell_type":"markdown","source":["### Notación en GloVe\n","\n","* v_main: the word vector for the main word in the co-occurrence\n","\n","* v_context: the word vector for the context word in the co-occurrence\n","\n","* b_main: bias scalar for main word\n","\n","* b_context: bias scalar for context word\n","\n","* gradsq_W_main: a vector storing the squared gradient history for the main word vector (for use in the AdaGrad update)\n","\n","* gradsq_W_context: a vector gradient history for the context word vector\n","\n","* gradsq_b_main: a scalar gradient history for the main word bias\n","\n","* gradsq_b_context: a scalar gradient history for the context word bias\n","\n","* cooccurrence: the Xij value for the co-occurrence pair, described at length above\n","\n","\n"]},{"metadata":{"id":"90oXlzA1NeR0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from scipy.sparse import lil_matrix\n","from math import log\n","from functools import partial"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mSHh_fNuNeR3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"9c1eb8f5-d64c-4eb0-c855-0b9621cc7ebd","executionInfo":{"status":"ok","timestamp":1530788603826,"user_tz":-120,"elapsed":443,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["def build_cooccur(vocab, corpus, vocab_size, window_size=10, min_count=5):\n","    \"\"\"\n","    Build a word co-occurrence list for the given corpus.\n","    This function is a tuple generator, where each element (representing\n","    a cooccurrence pair) is of the form\n","        (i_main, i_context, cooccurrence)\n","    where `i_main` is the ID of the main word in the cooccurrence and\n","    `i_context` is the ID of the context word, and `cooccurrence` is the\n","    `X_{ij}` cooccurrence value as described in Pennington et al.\n","    (2014).\n","    If `min_count` is not `None`, cooccurrence pairs where either word\n","    occurs in the corpus fewer than `min_count` times are ignored.\n","    \"\"\"\n","\n","    # Collect cooccurrences internally as a sparse matrix for passable\n","    # indexing speed; we'll convert into a list later\n","    print('vocab size', vocab_size)\n","    cooccurrences = lil_matrix((vocab_size, vocab_size),\n","                                      dtype=np.float64)\n","\n","    for i, line in tqdm(enumerate(corpus)):\n","        tokens = [t.text for t in line]\n","        token_ids = [vocab[word] for word in tokens]\n","\n","        for center_i, center_id in enumerate(token_ids):\n","            # Collect all word IDs in left window of center word\n","            context_ids = token_ids[max(0, center_i - window_size) : center_i]\n","            contexts_len = len(context_ids)\n","\n","            for left_i, left_id in enumerate(context_ids):\n","                # Distance from center word\n","                distance = contexts_len - left_i\n","\n","                # Weight by inverse of distance between words\n","                increment = 1.0 / float(distance)\n","\n","                # Build co-occurrence matrix symmetrically (pretend we\n","                # are calculating right contexts as well)\n","                cooccurrences[center_id, left_id] += increment\n","                cooccurrences[left_id, center_id] += increment\n","\n","    # Now yield our tuple sequence (dig into the LiL-matrix internals to\n","    # quickly iterate through all nonzero cells)\n","    for i, (row, data) in enumerate(zip(cooccurrences.rows,\n","                                                   cooccurrences.data)):\n","        if min_count is not None and vocab[id2w[i]] < min_count:\n","            continue\n","\n","        for data_idx, j in enumerate(row):\n","            if min_count is not None and vocab[id2w[j]] < min_count:\n","                continue\n","\n","            yield i, j, data[data_idx]"],"execution_count":3,"outputs":[]},{"metadata":{"id":"S-Dc_QjwNeR4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"25f9f6cd-ee83-4295-a0dc-3a3df0d10d99","executionInfo":{"status":"ok","timestamp":1530788797091,"user_tz":-120,"elapsed":441,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["def run_iter(vocab, data, learning_rate=0.05, x_max=100, alpha=0.75):\n","    \"\"\"\n","    Run a single iteration of GloVe training using the given\n","    cooccurrence data and the previously computed weight vectors /\n","    biases and accompanying gradient histories.\n","    `data` is a pre-fetched data / weights list where each element is of\n","    the form\n","        (v_main, v_context,\n","         b_main, b_context,\n","         gradsq_W_main, gradsq_W_context,\n","         gradsq_b_main, gradsq_b_context,\n","         cooccurrence)\n","    as produced by the `train_glove` function. Each element in this\n","    tuple is an `ndarray` view into the data structure which contains\n","    it.\n","    See the `train_glove` function for information on the shapes of `W`,\n","    `biases`, `gradient_squared`, `gradient_squared_biases` and how they\n","    should be initialized.\n","    The parameters `x_max`, `alpha` define our weighting function when\n","    computing the cost for two word pairs; see the GloVe paper for more\n","    details.\n","    Returns the cost associated with the given weight assignments and\n","    updates the weights by online AdaGrad in place.\n","    \"\"\"\n","\n","    global_cost = 0\n","\n","    # We want to iterate over data randomly so as not to unintentionally\n","    # bias the word vector contents\n","    shuffle(data)\n","\n","    for (v_main, v_context, b_main, b_context, gradsq_W_main, gradsq_W_context,\n","         gradsq_b_main, gradsq_b_context, cooccurrence) in data:\n","\n","        weight = (cooccurrence / x_max) ** alpha if cooccurrence < x_max else 1\n","\n","        # Compute inner component of cost function, which is used in\n","        # both overall cost calculation and in gradient calculation\n","        #\n","        #   $$ J' = w_i^Tw_j + b_i + b_j - log(X_{ij}) $$\n","        cost_inner = (v_main.dot(v_context)\n","                      + b_main[0] + b_context[0]\n","                      - log(cooccurrence))\n","\n","        # Compute cost\n","        #\n","        #   $$ J = f(X_{ij}) (J')^2 $$\n","        cost = weight * (cost_inner ** 2)\n","\n","        # Add weighted cost to the global cost tracker\n","        global_cost += 0.5 * cost\n","\n","        # Compute gradients for word vector terms.\n","        #\n","        # NB: `main_word` is only a view into `W` (not a copy), so our\n","        # modifications here will affect the global weight matrix;\n","        # likewise for context_word, biases, etc.\n","        grad_main = cost_inner * v_context\n","        grad_context = cost_inner * v_main\n","\n","        # Compute gradients for bias terms\n","        grad_bias_main = cost_inner\n","        grad_bias_context = cost_inner\n","\n","        # Now perform adaptive updates\n","        v_main -= (learning_rate * grad_main / np.sqrt(gradsq_W_main))\n","        v_context -= (learning_rate * grad_context / np.sqrt(gradsq_W_context))\n","\n","        b_main -= (learning_rate * grad_bias_main / np.sqrt(gradsq_b_main))\n","        b_context -= (learning_rate * grad_bias_context / np.sqrt(\n","                gradsq_b_context))\n","\n","        # Update squared gradient sums\n","        gradsq_W_main += np.square(grad_main)\n","        gradsq_W_context += np.square(grad_context)\n","        gradsq_b_main += grad_bias_main ** 2\n","        gradsq_b_context += grad_bias_context ** 2\n","\n","    return global_cost"],"execution_count":5,"outputs":[]},{"metadata":{"id":"3Mo93xqZNeR9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"bbb747b1-844a-422a-fb3a-2811bbd06762","executionInfo":{"status":"ok","timestamp":1530788794404,"user_tz":-120,"elapsed":494,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["def train_glove(vocab, cooccurrences, iter_callback=None, vector_size=100,\n","                iterations=25, **kwargs):\n","    \"\"\"\n","    Train GloVe vectors on the given generator `cooccurrences`, where\n","    each element is of the form\n","        (word_i_id, word_j_id, x_ij)\n","    where `x_ij` is a cooccurrence value $X_{ij}$ as presented in the\n","    matrix defined by `build_cooccur` and the Pennington et al. (2014)\n","    paper itself.\n","    If `iter_callback` is not `None`, the provided function will be\n","    called after each iteration with the learned `W` matrix so far.\n","    Keyword arguments are passed on to the iteration step function\n","    `run_iter`.\n","    Returns the computed word vector matrix `W`.\n","    \"\"\"\n","\n","    vocab_size = len(vocab)\n","\n","    # Word vector matrix. This matrix is (2V) * d, where N is the size\n","    # of the corpus vocabulary and d is the dimensionality of the word\n","    # vectors. All elements are initialized randomly in the range (-0.5,\n","    # 0.5]. We build two word vectors for each word: one for the word as\n","    # the main (center) word and one for the word as a context word.\n","    #\n","    # It is up to the client to decide what to do with the resulting two\n","    # vectors. Pennington et al. (2014) suggest adding or averaging the\n","    # two for each word, or discarding the context vectors.\n","    W = (np.random.rand(vocab_size * 2, vector_size) - 0.5) / float(vector_size + 1)\n","\n","    # Bias terms, each associated with a single vector. An array of size\n","    # $2V$, initialized randomly in the range (-0.5, 0.5].\n","    biases = (np.random.rand(vocab_size * 2) - 0.5) / float(vector_size + 1)\n","\n","    # Training is done via adaptive gradient descent (AdaGrad). To make\n","    # this work we need to store the sum of squares of all previous\n","    # gradients.\n","    #\n","    # Like `W`, this matrix is (2V) * d.\n","    #\n","    # Initialize all squared gradient sums to 1 so that our initial\n","    # adaptive learning rate is simply the global learning rate.\n","    gradient_squared = np.ones((vocab_size * 2, vector_size),\n","                               dtype=np.float64)\n","\n","    # Sum of squared gradients for the bias terms.\n","    gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n","\n","    # Build a reusable list from the given cooccurrence generator,\n","    # pre-fetching all necessary data.\n","    #\n","    # NB: These are all views into the actual data matrices, so updates\n","    # to them will pass on to the real data structures\n","    #\n","    # (We even extract the single-element biases as slices so that we\n","    # can use them as views)\n","    data = [(W[i_main], W[i_context + vocab_size],\n","             biases[i_main : i_main + 1],\n","             biases[i_context + vocab_size : i_context + vocab_size + 1],\n","             gradient_squared[i_main], gradient_squared[i_context + vocab_size],\n","             gradient_squared_biases[i_main : i_main + 1],\n","             gradient_squared_biases[i_context + vocab_size\n","                                     : i_context + vocab_size + 1],\n","             cooccurrence)\n","            for i_main, i_context, cooccurrence in cooccurrences]\n","\n","    for i in tqdm(range(iterations), desc='training'):\n","        \n","        cost = run_iter(vocab, data, **kwargs)\n","        if iter_callback is not None:\n","            iter_callback(W)\n","\n","    return W"],"execution_count":4,"outputs":[]},{"metadata":{"id":"os3ujglQNeSA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["cooccurrences = build_cooccur(w2id, quijote_docs, vocab_size=config['vocab_size'],\n","                             window_size=5,\n","                             min_count=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kyZZNQu8NeSC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":37},"outputId":"089ceca9-45f7-4435-dd0e-603bd5cbdad8","executionInfo":{"status":"ok","timestamp":1530788819299,"user_tz":-120,"elapsed":435,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["from random import shuffle\n","import itertools\n","import logging\n","\n","logger = logging.getLogger(\"glove\")"],"execution_count":6,"outputs":[]},{"metadata":{"id":"2DTxIkkgNeSE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["config = {\n","    'vocab_size': len(vocab),\n","    'embed_size': 100,\n","    'context_size':3\n","}\n","logger = logging.getLogger(\"glove\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V5KdguRgNeSG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["if True:\n","    word_embeddings = train_glove(w2id, cooccurrences,\n","                        vector_size=config['embed_size'],\n","                        iterations=100,\n","                        learning_rate=0.1)\n","    np.save('word_embeddings', word_embeddings)\n","else:\n","    word_embeddings = np.load('word_embeddings.npy')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i7n2Ah6JNeSH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":17},"outputId":"eb611db1-77fb-436a-bd01-dacd75c2671e","executionInfo":{"status":"ok","timestamp":1530788835459,"user_tz":-120,"elapsed":481,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["word_embeddings.shape"],"execution_count":6,"outputs":[]},{"metadata":{"id":"wy69vyu1GSfW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for i, row in enumerate(word_embeddings):\n","    word_embeddings[i, :] /= np.linalg.norm(row)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7lOh4p-nNeSK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"79c8d33b-0a2b-4e99-98f9-6d77c1e719a0"},"cell_type":"code","source":["emb_words = np.array(word_embeddings[config['vocab_size']:,])\n","print(emb_words.shape)\n","context_words = np.array(word_embeddings[config['vocab_size']:config['vocab_size']*2,])\n","print(context_words.shape)\n","word_embeddings = np.mean([emb_words, context_words], axis=0)\n","print(word_embeddings.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(25164, 100)\n","(25164, 100)\n","(25164, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"U2s9awMlNeSN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# compute pairwise distance matrix\n","if False:\n","    distance_matrix = euclidean_distances(word_embeddings)\n","    print(distance_matrix.shape)\n","\n","    # view contextually similar words\n","    similar_words = {search_term: [id2w[idx] for idx in distance_matrix[w2id[search_term]-1].argsort()[1:6]+1] \n","                       for search_term in ['Quijote', 'Mancha', 'escudero', 'Sancho']}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xb4G28mKNeSS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"fa5761e5-58a6-4241-a66c-0fa8dc5f15cb"},"cell_type":"code","source":["word_embeddings.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25164, 100)"]},"metadata":{"tags":[]},"execution_count":106}]},{"metadata":{"id":"vjnAD_3cNeSV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"8f6b96a2-d0df-4b00-bc56-ec48dcb40241"},"cell_type":"code","source":["w_id = w2id['Quijote']\n","print(w_id)\n","quijote_w = word_embeddings[w_id,].reshape(1,-1)\n","quijote_w.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["8870\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(1, 100)"]},"metadata":{"tags":[]},"execution_count":123}]},{"metadata":{"id":"RPgbKj9sNeSX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","k_close = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jfl-rU1ENeSa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["distances = cosine_similarity(quijote_w, word_embeddings).flatten()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L6_5yvsmNeSb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"32e1eb98-3720-4bb9-8037-e59a2e7016b2"},"cell_type":"code","source":["ind = np.argpartition(distances, -k_close)[-k_close:]\n","distances_top = distances[ind]\n","np.argsort(distances_top)\n","for _id in np.argsort(distances_top):\n","    print(id2w[ind[_id]], distances_top[_id])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Camila 0.4180056502353547\n","sentóse 0.4259783727708059\n","escudero 0.45016300866762365\n","hermano 0.45127875962076225\n","amo 0.45368394197308304\n","casen 0.46910466912129367\n","Teresa 0.4932793481066978\n","marido 0.5000075675864711\n","Sancho 0.5659655974218973\n","Panza 0.9999999999999998\n"],"name":"stdout"}]},{"metadata":{"id":"xOsWQYl2NeSe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"b8ec2ca9-1187-40e6-9ec7-a10c2296447c"},"cell_type":"code","source":["for w in ['Sancho', 'Panza', 'Rocinante', 'Dulcinea', 'ingenioso']:\n","    print('{}\\n---------'.format(w))\n","    w_id = w2id[w]\n","    w_vector = word_embeddings[w_id,].reshape(1,-1)\n","    distances = cosine_similarity(w_vector, word_embeddings).flatten()\n","    ind = np.argpartition(distances, -k_close)[-k_close:]\n","    distances_top = distances[ind]\n","    np.argsort(distances_top)\n","    for _id in np.argsort(distances_top):\n","        print(id2w[ind[_id]], distances_top[_id])\n","    print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sancho\n","---------\n","padre 0.3986807090799656\n","amigo 0.4413413117945051\n","Pallida 0.4721364595852338\n","APROBACIÓN 0.474137181166619\n","Soneto 0.47972340418665466\n","don 0.49837576999921923\n","Epitafio 0.5120628747940579\n","Quijote 0.5303728928664393\n","Panza 0.5659655974218973\n","Sancho 0.9999999999999998\n","\n","Panza\n","---------\n","Camila 0.4180056502353547\n","sentóse 0.4259783727708059\n","escudero 0.45016300866762365\n","hermano 0.45127875962076225\n","amo 0.45368394197308304\n","casen 0.46910466912129367\n","Teresa 0.4932793481066978\n","marido 0.5000075675864711\n","Sancho 0.5659655974218973\n","Panza 0.9999999999999998\n","\n","Rocinante\n","---------\n","buen 0.3634907377935789\n","volábanle 0.3653954218416763\n","ellos 0.3663850850603597\n","alhucema 0.3674997404700465\n","sus 0.3731778244271906\n","pecho 0.3759047645827094\n","sobre 0.38284440776936535\n","volver 0.39995210973155054\n","caballo 0.4253226551985809\n","Rocinante 1.0000000000000002\n","\n","Dulcinea\n","---------\n","corazón 0.37125082099209367\n","freno 0.3737632192001009\n","alma 0.37654391288598216\n","color 0.38836480253850436\n","Rodríguez 0.39510549022835983\n","del 0.4019294350809999\n","conciencia 0.40566969810639514\n","señora 0.5062058142965098\n","Toboso 0.6440246470547308\n","Dulcinea 0.9999999999999999\n","\n","ingenioso\n","---------\n","negocios 0.3833966774479635\n","Diego- 0.3852413958668557\n","velo 0.395835306038529\n","espejos 0.3969799834009456\n","habla 0.4150625657442673\n","palmito 0.41935687211357264\n","mostrádnosla 0.4202458301462084\n","conducís 0.4278976742177727\n","español 0.4402482328097133\n","ingenioso 1.0\n","\n"],"name":"stdout"}]},{"metadata":{"id":"SXQpOpQnNeSm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"ee274b31-d78b-49fc-c703-6bfdd0fb18db"},"cell_type":"code","source":["w_hombre = w2id['principe']\n","w_mujer = w2id['princesa']\n","w_quijote = w2id['rey']\n","\n","w_h = word_embeddings[w_hombre,].reshape(1,-1)\n","w_m = word_embeddings[w_mujer,].reshape(1,-1)\n","w_q = word_embeddings[w_quijote,].reshape(1,-1)\n","\n","w_result = w_q - w_h + w_m\n","\n","distances = cosine_similarity(w_result, word_embeddings).flatten()\n","ind = np.argpartition(distances, -k_close)[-k_close:]\n","distances_top = distances[ind]\n","np.argsort(distances_top)\n","for _id in np.argsort(distances_top):\n","    print(id2w[ind[_id]], distances_top[_id])\n","print()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'w2id' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7411e2f9796a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_hombre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'principe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw_mujer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'princesa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw_quijote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rey'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw_hombre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'w2id' is not defined"]}]}]}