{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word Embeddings.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["hRt_Y3_UpPon","q2ypGdYWpMFo","WHvr2ZDgpU_z","D_Pg29vHpYg7","QpEpFY5Kpc71","ElRtLX60pftQ","ABtEOOnNpi_8"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wCLgSWjzNeQP","colab_type":"text"},"cell_type":"markdown","source":["# Intro to class\n","\n","\n","[word embeddings colah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)"]},{"metadata":{"id":"NoWTG-caNeQZ","colab_type":"text"},"cell_type":"markdown","source":["# Word Embeddings\n","\n","Cuando empezó esto de los word embeddings? Hace algo de tiempo, aquí por ejemplo. [Distributed Representations](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2841&context=compsci)\n","\n","![Word Embeddings](https://qph.fs.quoracdn.net/main-qimg-e8b83b14d7261d75754a92d0d3605e36 \"Word Embeddings\")\n","\n","Los word embeddings son vectores densos que representan tokens, aunque el término es word, sería más correcto usar token embeddings.\n","\n","Hay varias implementaciones, como CBOW o Skip-Gram (del famoso word2vec) o Global Vectors (GloVe). \n","\n","Són una solución al problema de dimensionalidad que comentamos el primer día de clase.\n","\n","### Vale pero que son realmente los valores de los word embeddings?\n","\n","Los valores de los word embeddings representan \"clases\", aunque es muy complicado estar 100% seguro de que clases son, sobretodo porque cada vez que entrenemos word embeddings, el modelo puede aprender unas u otras clases. Con lo cuál, hay que olvidar-se un poco del concepto de vector dónde la posición 1 siempre tendremos la clase 1, y en la posición 2 la clase 2.\n","\n","![](https://cdn-images-1.medium.com/max/1600/1*mLrheV1nGz7XemDAVRcZ4A.png)\n","\n","Pensad que cuando escogemos la dimensionalidad de estos, ya estamos fijando cuántas clases puede aprender, y además, cada dimensionalidad pueden ser una mezcla de clases. Así pues, y el objetivo de aprender word vectors, es el siguiente:\n","\n","> *Rezamos para que palabras \"similares\" tengan vectores similares*. Es decir a representar palabras por su semantismo.\n","\n","### Recordatorio:"]},{"metadata":{"id":"MuV18XhINeQf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import io"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ogoLmh-4NeQj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"dsAfLvSrNeQn","colab_type":"text"},"cell_type":"markdown","source":["Estas representaciones son simples features, pero podemos mejorar de alguna forma? Si quisieramos representar un documento con la ultima representación y ngrams por ejemplo, creariamos un array tal que asi:"]},{"metadata":{"id":"_qestvdJNeQo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"R0TI-XgFNeQu","colab_type":"text"},"cell_type":"markdown","source":["## Problemas que arreglamos\n","\n","Los shapes de las representaciones son muy indicativas del problema que queremos resolver. Es decir, si ahora en lugar de una frase, queremos representar un documento de 400 posiciones, y que tiene 20k features? Por cada documento generaremos arrays de shape=(400, 20000), lo cual se hace insostenible a medida de que hagamos crecer las features. \n","\n","Ahora que ya habeis visto la cantidad de datos necesarios para ciertos problemas como computer vision, texto no ha sido indiferente a la revolución del \"big data\". Aún así, los word embeddings NO son deep learning, pero si se usan en él. Sás las features más usadas para texto en deep learning.\n","\n","### Semántica\n","\n","Además muchos problemas de NLP se basan basicamente en la semantica de estos tokens, y hasta la fecha no hemos visitado demasiado la semantica de las palabras. Usando Word Embeddings podremos boostear nuestras features y consecuentemente nuestros outputs de los algoritmos. \n","\n","Haremos uns dummy implementación del CBOW y GloVe, pero podéis encontrar vectores ya entrenados, o algoritmos implementados de forma eficiente en librerías como gensim o spaCy. \n","\n","CBOW esta basado en redes neuronales, y GloVe en factorización de matríces.\n","\n","<div align=\"center\">\n","    ![cbow.png](https://adriancolyer.files.wordpress.com/2016/04/word2vec-cbow.png =500x)\n","</div>"]},{"metadata":{"id":"hRt_Y3_UpPon","colab_type":"text"},"cell_type":"markdown","source":["### Load Data"]},{"metadata":{"id":"esd2U8EbNeQy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":54},"outputId":"4dc8babe-59d0-4e85-d5b8-b29c0ecdad55","executionInfo":{"status":"ok","timestamp":1530778921611,"user_tz":-120,"elapsed":5923,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["!pip install tqdm\n","from tqdm import tqdm\n","\n","!pip install -q pydot\n","\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.4)\r\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"imEOahUTNeQ2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install spacy\n","!python -m spacy download es_core_news_sm\n","\n","import spacy\n","\n","nlp = spacy.load('es_core_news_sm', disable= ['ner', 'parser'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iMKO-xPcYRAH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":91},"outputId":"53d3530d-9188-4e71-d745-f43c0b88648b","executionInfo":{"status":"ok","timestamp":1530776699551,"user_tz":-120,"elapsed":23974,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-fd2f5b79-582b-4718-84a2-906639808096\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-fd2f5b79-582b-4718-84a2-906639808096\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving quijote.txt to quijote (1).txt\n","User uploaded file \"quijote.txt\" with length 2141457 bytes\n"],"name":"stdout"}]},{"metadata":{"id":"oBfkFoD4NeQ4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# construir el corpus\n","quijote = uploaded['quijote.txt']\n","quijote = io.StringIO(quijote.decode('utf-8')).getvalue()\n","\n","quijote = quijote.split('.')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q2ypGdYWpMFo","colab_type":"text"},"cell_type":"markdown","source":["### Dataset Preparation"]},{"metadata":{"id":"Yo0ZGqBQdgqG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"59f5342e-5c01-4e15-9ffa-8e49e5fed0e8","executionInfo":{"status":"ok","timestamp":1530777708447,"user_tz":-120,"elapsed":525,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["len(quijote)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8210"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"qYgEcEEtZOFE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":435},"outputId":"3607f665-f220-43fa-a6c5-848423449e67","executionInfo":{"status":"ok","timestamp":1530777776226,"user_tz":-120,"elapsed":33618,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["quijote_docs = []\n","for doc in tqdm(nlp.pipe(quijote, batch_size=1000, n_threads=4)):\n","    quijote_docs.append(doc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\n","0it [00:00, ?it/s]\u001b[A\u001b[A\n","\n","1it [00:04,  4.64s/it]\u001b[A\u001b[A\n","\n","1001it [00:08, 122.66it/s]\u001b[A\u001b[A\n","\n","2001it [00:12, 164.90it/s]\u001b[A\u001b[A\n","\n","3001it [00:16, 180.64it/s]\u001b[A\u001b[A\n","\n","4001it [00:20, 194.90it/s]\u001b[A\u001b[A\n","\n","5001it [00:24, 203.89it/s]\u001b[A\u001b[A\n","\n","6001it [00:28, 211.70it/s]\u001b[A\u001b[A\n","\n","7001it [00:32, 217.96it/s]\u001b[A\u001b[A\n","\n","8001it [00:32, 243.00it/s]\u001b[A\u001b[A\n","\n","8210it [00:32, 249.22it/s]\u001b[A\u001b[A"],"name":"stderr"}]},{"metadata":{"id":"QgrfPFjBNeQ9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZTVUzlwlNeRB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ERQitSr6NeRD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"k6kqwK5eNeRH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"WHvr2ZDgpU_z","colab_type":"text"},"cell_type":"markdown","source":["### Generate Dataset"]},{"metadata":{"id":"_He3tIjjNeRL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"QVdRe_LNNeRO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"D_Pg29vHpYg7","colab_type":"text"},"cell_type":"markdown","source":["### Create Model"]},{"metadata":{"id":"W5eNKTy7NeRU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"15c5ed06-ea54-4a18-d441-a737b8e3abce","executionInfo":{"status":"ok","timestamp":1530793523600,"user_tz":-120,"elapsed":1603,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","\n","from sklearn.metrics.pairwise import euclidean_distances"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"qHo-PIwGNeRY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IyRST4w1NeRi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"QpEpFY5Kpc71","colab_type":"text"},"cell_type":"markdown","source":["### Train"]},{"metadata":{"id":"sM53ri2QNeRm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ElRtLX60pftQ","colab_type":"text"},"cell_type":"markdown","source":["### Get word embeddings"]},{"metadata":{"id":"jAQGZYVmNeRn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"tYnn6QQ_NeRq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ABtEOOnNpi_8","colab_type":"text"},"cell_type":"markdown","source":["### Test"]},{"metadata":{"id":"pcwLb7G7NeRs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"O3b0utmpNeRz","colab_type":"text"},"cell_type":"markdown","source":["## GloVe\n","\n","El algoritmo que veremos se basa en la siguiente idea:\n","\n","<div align=\"center\">\n","![](https://image.slidesharecdn.com/analyticssummitnov082013final-131114141831-phpapp01/95/analytics-summit-2013-25-638.jpg?cb=1384439169 =300x)\n","</div>\n","\n","Es una idea que ya hemos visto presentada en formatos diferentes. Cuando entrenamos el Language Modeling, al final, estabamos asignando mayores probabilidades a palabras o tokens que aparecían en un contexto concreto, sólo que en ese caso, el contexto era sólo del pasado.\n","\n","Vamos a ver una implementación \"sencilla\" de GloVe. Como hemos comentado, está basada en dos cosas, la primera, en la factorización de matrices. No entraremos en detalles de que es la factorización de matrices, nos basta con saber que en el caso de GloVe, nuestro objetivo será, dado una matriz de co-ocurrencias, obtener dos matrices que multiplicadas reproduzcan la original.\n","\n","![glove.png](https://cdn-images-1.medium.com/max/800/1*UNtsSilztKXjLG99VXxSQw.png)\n","\n","La matriz lila, será nuestra matriz de co-ocurrencias. ¿Qué es una matriz de coocurrencias? Tan fácil como ir sumando, dado un contexto, esas palabras que van aparenciendo entre si. Aquí tenéis un ejemplo de su uso.\n","\n","![](https://cdn-images-1.medium.com/max/1600/0*Yl7I7bH52zk8m_8R.)\n","\n","Primera Cosa en la que tenemos que fijarnos. Que valor tienen words y context? Es el mismo valor?\n","\n","Y si es así, que hacemos con las dos matrices que generamos?\n","\n","#### Algoritmo\n","\n","![](https://i.imgur.com/HCo2ZwE.png)\n"]},{"metadata":{"id":"mX5hgPwMFaSY","colab_type":"text"},"cell_type":"markdown","source":["### Notación en GloVe\n","\n","* v_main: the word vector for the main word in the co-occurrence\n","\n","* v_context: the word vector for the context word in the co-occurrence\n","\n","* b_main: bias scalar for main word\n","\n","* b_context: bias scalar for context word\n","\n","* gradsq_W_main: a vector storing the squared gradient history for the main word vector (for use in the AdaGrad update)\n","\n","* gradsq_W_context: a vector gradient history for the context word vector\n","\n","* gradsq_b_main: a scalar gradient history for the main word bias\n","\n","* gradsq_b_context: a scalar gradient history for the context word bias\n","\n","* cooccurrence: the Xij value for the co-occurrence pair, described at length above\n","\n","\n"]},{"metadata":{"id":"90oXlzA1NeR0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from scipy.sparse import lil_matrix\n","from math import log\n","from functools import partial"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mSHh_fNuNeR3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def build_cooccur(vocab, corpus, vocab_size, window_size=10, min_count=5):\n","    return \"Co-ocurrence Matrix\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"S-Dc_QjwNeR4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def run_iter(vocab, data, learning_rate=0.05, x_max=100, alpha=0.75):\n","\n","    global_cost = 0\n","\n","    # We want to iterate over data randomly so as not to unintentionally\n","    # bias the word vector contents\n","    shuffle(data)\n","\n","    for (v_main, v_context, b_main, b_context, gradsq_W_main, gradsq_W_context,\n","         gradsq_b_main, gradsq_b_context, cooccurrence) in data:\n","        \n","        #FORWARD PASS\n","        weight = \"Programar esto\"\n","\n","        cost_inner = \"y esto\"\n","\n","        cost = \"calcular el coste\"\n","\n","        # Add weighted cost to the global cost tracker\n","        global_cost += 0.5 * cost\n","\n","        #BACKWARD/OPTIMIZATION PASS. Fuera del contenido, así que no lo implementaremos.\n","        # El algoritmo de optimización se llama AdaGrad, también disponible en Keras\n","        grad_main = cost_inner * v_context\n","        grad_context = cost_inner * v_main\n","\n","        # Compute gradients for bias terms\n","        grad_bias_main = cost_inner\n","        grad_bias_context = cost_inner\n","\n","        # Now perform adaptive updates\n","        v_main -= (learning_rate * grad_main / np.sqrt(gradsq_W_main))\n","        v_context -= (learning_rate * grad_context / np.sqrt(gradsq_W_context))\n","\n","        b_main -= (learning_rate * grad_bias_main / np.sqrt(gradsq_b_main))\n","        b_context -= (learning_rate * grad_bias_context / np.sqrt(\n","                gradsq_b_context))\n","\n","        # Update squared gradient sums\n","        gradsq_W_main += np.square(grad_main)\n","        gradsq_W_context += np.square(grad_context)\n","        gradsq_b_main += grad_bias_main ** 2\n","        gradsq_b_context += grad_bias_context ** 2\n","\n","    return global_cost"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Mo93xqZNeR9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train_glove(vocab, cooccurrences, iter_callback=None, vector_size=100,\n","                iterations=25, **kwargs):\n","    \"\"\"\n","    Train GloVe vectors on the given generator `cooccurrences`, where\n","    each element is of the form\n","        (word_i_id, word_j_id, x_ij)\n","    where `x_ij` is a cooccurrence value $X_{ij}$ as presented in the\n","    matrix defined by `build_cooccur` and the Pennington et al. (2014)\n","    paper itself.\n","    If `iter_callback` is not `None`, the provided function will be\n","    called after each iteration with the learned `W` matrix so far.\n","    Keyword arguments are passed on to the iteration step function\n","    `run_iter`.\n","    Returns the computed word vector matrix `W`.\n","    \"\"\"\n","\n","    vocab_size = len(vocab)\n","    \n","    # Matrix vectors\n","    W = \"Inicializar esto\"\n","\n","    #biases\n","    biases = \"inicializar esto\"\n","\n","    \n","    # Igual que antes, no implementaremos esta parte.\n","    \n","    gradient_squared = np.ones((vocab_size * 2, vector_size),\n","                               dtype=np.float64)\n","\n","    # Sum of squared gradients for the bias terms.\n","    gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n","\n","    data = [(W[i_main], W[i_context + vocab_size],\n","             biases[i_main : i_main + 1],\n","             biases[i_context + vocab_size : i_context + vocab_size + 1],\n","             gradient_squared[i_main], gradient_squared[i_context + vocab_size],\n","             gradient_squared_biases[i_main : i_main + 1],\n","             gradient_squared_biases[i_context + vocab_size\n","                                     : i_context + vocab_size + 1],\n","             cooccurrence)\n","            for i_main, i_context, cooccurrence in cooccurrences]\n","\n","    for i in tqdm(range(iterations), desc='training'):\n","        \n","        cost = run_iter(vocab, data, **kwargs)\n","        if iter_callback is not None:\n","            iter_callback(W)\n","\n","    return W"],"execution_count":0,"outputs":[]},{"metadata":{"id":"os3ujglQNeSA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["cooccurrences = build_cooccur(w2id, quijote_docs, vocab_size=config['vocab_size'],\n","                             window_size=5,\n","                             min_count=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kyZZNQu8NeSC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from random import shuffle\n","import itertools\n","import logging\n","\n","logger = logging.getLogger(\"glove\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2DTxIkkgNeSE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["config = {\n","    'vocab_size': len(vocab),\n","    'embed_size': 100,\n","    'context_size':3\n","}\n","logger = logging.getLogger(\"glove\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V5KdguRgNeSG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"i7n2Ah6JNeSH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"wy69vyu1GSfW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7lOh4p-nNeSK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"U2s9awMlNeSN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xb4G28mKNeSS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vjnAD_3cNeSV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"RPgbKj9sNeSX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","k_close = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jfl-rU1ENeSa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"L6_5yvsmNeSb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xOsWQYl2NeSe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"SXQpOpQnNeSm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}