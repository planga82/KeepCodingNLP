{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature representation.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python [conda env:MythNLP]","language":"python","name":"conda-env-MythNLP-py"}},"cells":[{"metadata":{"id":"mZ6kAyglcwps","colab_type":"text"},"cell_type":"markdown","source":["# Text Representation\n","## BoW and n-grams\n","\n","Empezaremos por la representación más clave en NLP, que es la Bolsa de palabras.\n","\n","La bolsa de palabras es extremadamente fácil de crear y extremadamente usada como input vector en muchísimos algoritmos de lenguaje. Nos permitirá recuperar numpy y ver la primera interacción con sklearn.\n","\n","## ¿En qué consiste?\n","\n","Consiste en representar un input textual como un vector. ¿Qué valor tendrá cada casilla de un vector? Fácil, primero generaremos un vocabulario de nuestro train data. Luego, asignaremos un entero a cada palabra y ese entero será el índice de este vector. Finalmente, por cada frase, asignaremos en cada posición del vector el número de veces que aparece en frases.\n","\n","![](https://i.imgur.com/EPG2zSK.jpg)\n","\n","Vamos a ello paso a paso."]},{"metadata":{"id":"C1_36_JLcwp1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["documents = [\n","    'frase en castellano',\n","    'english sentence',\n","    'esta frase no esta en english',\n","    'estos documentos tienen muy poco sentido',\n","    'el documento es un conjunto de frases',\n","    'Vim es mucho mejor que emacs.',\n","    'tabs vs spaces'\n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pZtG85sTcwqC","colab_type":"text"},"cell_type":"markdown","source":["### ¿Usaremos todas las palabras?\n","\n","No siempre. Ver stop-words.\n","\n","¿Alguna cosilla más en el vocabulary? UNK, PAD tokens."]},{"metadata":{"id":"AjvGwGCOcwqD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate_vocabulary_maps(docs):\n","    vocabulary = {}\n","    inverse_vocabulary = {}\n","    for doc in documents:\n","        for token in doc.split(' '):\n","            if token not in vocabulary:\n","                vocabulary[token] = len(vocabulary)\n","                inverse_vocabulary[len(inverse_vocabulary)] = token\n","    return vocabulary, inverse_vocabulary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ARZKqbg4cwqI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"c5b63bc0-92ca-41b4-ffd6-1bf47a7d11b5"},"cell_type":"code","source":["vocabulary, inverse_vocabulary = generate_vocabulary_maps(documents)\n","print(vocabulary)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'muy': 10, 'tienen': 9, 'el': 13, 'mucho': 21, 'frases': 19, 'que': 23, 'en': 1, 'emacs.': 24, 'english': 3, 'vs': 26, 'esta': 5, 'frase': 0, 'no': 6, 'documentos': 8, 'de': 18, 'conjunto': 17, 'sentence': 4, 'castellano': 2, 'estos': 7, 'mejor': 22, 'documento': 14, 'spaces': 27, 'Vim': 20, 'es': 15, 'tabs': 25, 'un': 16, 'sentido': 12, 'poco': 11}\n"],"name":"stdout"}]},{"metadata":{"id":"KLKO-oXgcwqT","colab_type":"text"},"cell_type":"markdown","source":["## Introducción de numpy en el curso.\n"]},{"metadata":{"id":"fQOv2emUcwqU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"79gwPfezcwqb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"7b169cc6-ae27-4a9d-c70d-92ce5133e248"},"cell_type":"code","source":["new_representation_np = np.zeros((len(vocabulary)), dtype='int32')\n","new_representation_np"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"Wz82TBuxcwqh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["string = 'documento en castellano'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ISK5p1s8cwql","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def transform(x, vocab):\n","    assert type(x) == str, 'wrong type. x must be a sentence'\n","    new_representation_np = np.zeros((len(vocab)), dtype='int32')\n","    idx = [vocab[token] for token in x.split(' ')]\n","    new_representation_np[idx] += 1\n","    return new_representation_np    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"seXE7qFGcwqo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"1cdcd22e-e0ac-44de-fbff-14050b96d089"},"cell_type":"code","source":["transform(string, vocab=vocabulary)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"LM-d-tkdcwqt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["new_docs = [\n","    'este documento es de programacion en castellano',\n","    'castellano documento en en',\n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EJy88m7Fcwqx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def itransform(X, vocab):\n","    assert type(X)==list, 'X must be a list'\n","    rows= len(X)\n","    rep = np.zeros((rows, len(vocab)), dtype='int32')\n","    for i, x in enumerate(X):\n","        tokens = [vocab[token] for token in x.split(' ') if token in vocab]\n","        for t in tokens:\n","            rep[i, t] += 1\n","    return rep"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qI8eLUQ0cwq1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"71e516dd-137a-47df-a3e4-d209f5bed9ec"},"cell_type":"code","source":["itransform(new_docs, vocabulary)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0],\n","       [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"2Qrxt8r5cwq4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["docs = documents + new_docs\n","count_docs = itransform(docs, vocabulary)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0yeTc90jcwq6","colab_type":"text"},"cell_type":"markdown","source":["Ahora que ya hemos visto lo rápido que es preparar un modelo de Bolsa de palabras, habría que hacer un pequeno análisis de los problemas que puede tener.\n","\n","Ver Bow Issues en OneNote.\n","\n","¿Cuán grande puede ser un vocabulario?\n","\n","El español tiene 100.000 palabras aproximadamente, sin contar con conjugaciones verbales.\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/a29W4JC.png)\n","</div>"]},{"metadata":{"id":"5HAiBQZz1CJp","colab_type":"text"},"cell_type":"markdown","source":["# N-grams\n","\n","Una de las posibles soluciones al modelo de Bolsa de Palabras es una ampliación bastante natural. En lugar de coger palabras sueltas, cogemos n-palabras consecutivas. A esto lo denominamos n-grams.\n","\n","Los bigrams, y trigrams son modelos de features bastante usados, hasta 5-grams, y no son de uso exclusivo. Es decir, podemos concatenar el input de una frase usando [unigram_vector, bigram_vector, trigram_vector].\n","\n","p.e.:\n","\n","> ¿Qué puedo hacer?\n","\n","> Me dijo que quería hacer algo.\n","\n","En un problema de clasificación, quizás es importante saber órdenes, aunque sean parciales.\n","\n","La siguiente quote proviene de Google, cuando hicieron una release de los n-grams que calcularon en la web.\n","\n","> \n","We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 times.\n","\n","\n","\n","Ahora veremos como se implementa un simple modelo de n-grams."]},{"metadata":{"id":"ktSqGmOqcwq7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\"\"\"\n","<SOS>\n","<EOS>\n","\"\"\"\n","from collections import Counter\n","\n","def compute_ngrams(docs, min_n=1, max_n=2, ngrams={}):\n","    assert max_n>min_n, 'max ngram must be bigger than min ngram'\n","    ngrams = ngrams if ngrams else {i:Counter() for i in range(min_n, max_n+1)} \n","    for doc in docs:\n","        doc = '<SOS> ' + doc + ' <EOS>'\n","        tokenized_doc = doc.split(' ')\n","        for ix in range(len(tokenized_doc)):\n","            ngrams_doc = [\" \".join(tokenized_doc[ix:ix+i]) for i in range(min_n, max_n+1) if ix+i < len(tokenized_doc)+1]\n","            for i, ngram in enumerate(ngrams_doc):\n","                ngrams[i+1][ngram]+=1\n","    return ngrams   \n","\n","def update_ngrams(docs, ngrams, min_n=False, max_n=False):\n","    min_n = min_n if min_n else min(ngrams.keys())\n","    max_n = max_n if max_n else max(ngrams.keys())\n","    return compute_ngrams(docs, min_n, max_n, ngrams)\n","    \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"GoBdtG4Jcwq9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"86fb028f-c24a-4ba0-a1f8-80f4f79f82f8"},"cell_type":"code","source":["ngrams = compute_ngrams(docs)\n","for k in ngrams.keys():\n","    print(ngrams[k].most_common(2))\n","ngrams = update_ngrams(docs, ngrams)\n","for k in ngrams.keys():\n","    print(ngrams[k].most_common(2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('<EOS>', 9), ('<SOS>', 9)]\n","[('documento es', 2), ('castellano <EOS>', 2)]\n","[('<EOS>', 18), ('<SOS>', 18)]\n","[('documento es', 4), ('castellano <EOS>', 4)]\n"],"name":"stdout"}]},{"metadata":{"id":"8l_7VRU3cwrA","colab_type":"text"},"cell_type":"markdown","source":["En este punto ya tenemos unas features básicas para poder realizar tareas de clasificación de textos con un clasificador como bayes.\n","\n","Solo nos quedaría mapear esto a un numpy array. En estos ejemplos todo es magnífico. Pero, ¿y en la realidad? ¡Vectores enormes! ¡Explosión de features! ¿Qué pasa con los vocabularios muy extensos?"]},{"metadata":{"id":"olw0KvQ_cwrD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate_feature_maps(features):\n","    \"\"\"\n","    Adapatar a las estructuras de cada uno, i.e cada proyecto.\n","    Podriamos estar interesados en el inverso.   \n","    \"\"\"\n","    feature_map = {}\n","    for ngram in features.values():\n","        for token in ngram.keys():\n","            feature_map[token] = len(feature_map)\n","    return feature_map"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d_Ek0hFzcwrE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["feature_map = generate_feature_maps(ngrams)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Q2Nxq1vcwrH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"68f2cd03-f6ff-42fe-9431-e6555a572f56"},"cell_type":"code","source":["print('Hay', len(docs), 'frases')\n","print('Hemos generado', len(feature_map), 'features')\n","print('Solo teniamos', len(set([token for doc in docs for token in doc.split(' ')])), 'palabras unicas')\n","print('Comprobacion con ngrams', len(ngrams[1])-2, 'quitando <SOS> y <EOS>')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Hay 9 frases\n","Hemos generado 82 features\n","Solo teniamos 30 palabras unicas\n","Comprobacion con ngrams 30 quitando <SOS> y <EOS>\n"],"name":"stdout"}]},{"metadata":{"id":"Guv_7UVEcwrK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate_feature_vector(docs, vocab, min_n=1, max_n=3):\n","    feature_vector = np.zeros((len(docs), len(vocab)), dtype='int32')\n","    for i, doc in enumerate(docs):\n","        doc = '<SOS> ' + doc + ' <EOS>' # tendria que estar hecho en el preproceso de la frase esto!\n","        tokenized_doc = doc.split(' ')\n","        print(tokenized_doc)\n","        for ix in range(len(tokenized_doc)):\n","            ngrams_doc = [\" \".join(tokenized_doc[ix:ix+i]) for i in range(min_n, max_n+1) if ix+i < len(tokenized_doc)+1]\n","            print(ngrams_doc)\n","            maped = [vocab[ngram] for ngram in ngrams_doc if ngram in vocab]\n","            for ngram in maped:\n","                feature_vector[i, ngram] += 1\n","    return feature_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0lSQ_iWVcwrN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"59484b6a-f97b-4c94-a502-a046b304cd46"},"cell_type":"code","source":["test_docs = ['nuevo documento']\n","test_vector = generate_feature_vector(test_docs, feature_map)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['<SOS>', 'nuevo', 'documento', '<EOS>']\n","['<SOS>', '<SOS> nuevo', '<SOS> nuevo documento']\n","['nuevo', 'nuevo documento', 'nuevo documento <EOS>']\n","['documento', 'documento <EOS>']\n","['<EOS>']\n"],"name":"stdout"}]},{"metadata":{"id":"OHlrAJstcwrP","colab_type":"text"},"cell_type":"markdown","source":["Se generan vectores enormes con muchos 0 y pocos 1. Más adelante veremos cómo dejar de tener vectores tan grandes. Hay dos tipos de representación en vectores: sparse, que es la que tenemos aquí, y dense, que la veremos en PCA/SVD o word embeddings.  "]},{"metadata":{"id":"Lg-F0zQlcwrP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"23094eed-e80d-4cf9-d764-718ee0afb444"},"cell_type":"code","source":["from scipy.sparse import csr_matrix\n","sparsified = csr_matrix(test_vector)\n","print(sparsified)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  (0, 6)\t1\n","  (0, 24)\t1\n","  (0, 31)\t1\n"],"name":"stdout"}]},{"metadata":{"id":"Fx0J_orXcwrU","colab_type":"text"},"cell_type":"markdown","source":[" <div><font color=\"red\">\n","\n","NLP tiene un gran problema que acabamos de ver. El vocabulario es ~infinito.</font>\n","\n","El vocabulario se puede generar vocabulario usando tu train data, tambien se puede generar vocabulario con las palabras más usadas en un idioma. Es complicadísimo generar un vocabulario general para una tarea que contenga NLP.\n","</div>\n","\n","De hecho, a lo largo de las sesiones, veremos que pasa con todas aquellas palabras Out Of Vocabulary (OOV). Es importante que tengamos en cuenta que esto nos va a ocurrir, y como modelamos nuestros problemas para que su afectación sea mínima, o nos permita generalizar de forma óptima.\n","\n","\n"," *Es MUY importante en nuestros pipelines de NLP intentar controlar el vocabulario de alguna manera.*\n","\n","Hay varias maneras de hacerlo e iremos viendolas a lo largo de las sesiones. Ya hemos visto una (distancia de edición) para intentar corregir errores ortográficos. En problemas de clasificación con metodos de representación más clásicos, usaremos smoothing. En deep learning hay la tendencia de \"unkificarlo todo\", es decir todos aquellos tokens que desconocemos, tratarlos con un token especial < UNK >\n","\n","A continuación veremos lo mismo de antes con la libreria sklearn, muy recomendable tirar de ella tanto para feature extraction como para clasificación."]},{"metadata":{"id":"5Le5PDVxcwrV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AaH6gipPcwrY","colab_type":"text"},"cell_type":"markdown","source":["http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"]},{"metadata":{"id":"RzxcULhDcwrZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"53ce43b8-03d2-4ddb-db2b-06be24465586"},"cell_type":"code","source":["vectorizer = CountVectorizer(ngram_range=(1,3))\n","vectorizer.fit(documents)\n","vectorizer.get_params()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'analyzer': 'word',\n"," 'binary': False,\n"," 'decode_error': 'strict',\n"," 'dtype': numpy.int64,\n"," 'encoding': 'utf-8',\n"," 'input': 'content',\n"," 'lowercase': True,\n"," 'max_df': 1.0,\n"," 'max_features': None,\n"," 'min_df': 1,\n"," 'ngram_range': (1, 3),\n"," 'preprocessor': None,\n"," 'stop_words': None,\n"," 'strip_accents': None,\n"," 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n"," 'tokenizer': None,\n"," 'vocabulary': None}"]},"metadata":{"tags":[]},"execution_count":62}]},{"metadata":{"id":"xmyWI_L4cwrc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"df72517d-1926-457f-ad49-e5665119f105"},"cell_type":"code","source":["print(vectorizer.transform(['nuevo documento']).toarray())\n","print(vectorizer.transform(['nuevo documento']))\n","vectorizer.transform(['nuevo documento'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","  (0, 6)\t1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<1x73 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 1 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"rWXee2-Qcwrf","colab_type":"text"},"cell_type":"markdown","source":["Es muy fácil de usar, bastante eficiente y 100% integrable con otras librerías, ya que devuelve numpy arrays, que son la base para muchas otras librerías.\n","\n","En caso de que el vocabulario sea muy grande, en lugar de utilizar CountVectorizer usaríamos http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer .\n","\n","Es más eficiente a nivel de memoria, pero con el drawback de que no podríamos sacar el inverso de las features que calcula. De todas formas, pronto veremos que los one hot vectors se usan cada vez menos. \n","\n","Normalmente, todos estas funciones como el CountVectorizer, o cualquier algoritmo que queramos implementar, debería permitir-nos poder usar tanto palabras, como carácteres como features. Aquí ya podemos verlo, y lo veremos mucho mejor en la siguiente sesión!"]}]}