{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro NLP.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python [conda env:MythNLP]","language":"python","name":"conda-env-MythNLP-py"}},"cells":[{"metadata":{"id":"MEt5Bn-h79fr","colab_type":"text"},"cell_type":"markdown","source":["## Bienvenidos\n","\n","Bienvenidos a todos al curso de NLP & Search orientado a desarrolladores, mi nombre es **David**, y mi correo electronico **torrejonmoya.david@gmail.com**\n","\n","Actualmente trabajo en Goldenspear, una empresa a caballo de la inteligencia artificial y el mundo de la moda. Mi trabajo esta repartido en 65% implementación de producto y 35% research en NLP. Entre otras cosas, he desarrollado interfícies basadas en lenguaje natural para búsquedas en nuestra plataforma y, actualmente estoy desarrollando un asistente de moda.\n","\n","Las clases estan basadas 100% en notebooks, de hecho, usaremos el collaboratory para todo. No usaremos grandes datasets, así que con las herramientas que tenemos a mano, tendremos más que suficiente. Al iniciar cada clase, intentaré proporcionaros con algo de material extra para los que estéis interesados en aprofundir más. No tiene porque ser material más técnico o complejo, así que os animo a todos los que podáis que le echéis un ojo.\n","\n","Entiendo que todos habéis cursado:\n","    1. Algebra. \n","    2. ML 101.\n","    3. Deep Learning.\n","    \n","Nos apoyaremos en todo el conocimiento obtenido a lo largo del curso para aplicarlo en esta clase.\n","\n","**El curso se hará integramente en python, usando librerías open source del magnífico stack que existe. La mayoría de librerias que usaremos, ya las habeis usado!**\n","\n","Os dejo también antes de empezar, 3 libros que me parece interesantes. El primero es quizás el libro teórico más referenciado en la actualidad. El segundo es un libro bastante nuevo, de hecho creo que es del 2017, que mezcla muy bien Deep Learning y NLP. El tercero es más práctico que teórico, y es un libro correcto para empezar.\n","    \n","    \n","*Lecturas Recomendadas:    *\n","1. [Speech and Language Processing](http://web.stanford.edu/~jurafsky/slp3/): Quizás el libro más clasico de NLP. Es un libro de referencia, contiene material muy valuoso, y con el tiempo lo actualizan. Estan 100% online si no quereis comprarlo. \n","    \n","2. [Neural Network Methods for NLP](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984) : Mágnifico libro en la intersección entre Deep Learning y NLP. Tiene un repaso de álgebra muy bueno al inicio del libro.    \n","    \n","3.  [Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495/ref=pd_lpo_sbs_14_img_0/131-4110889-0070208?_encoding=UTF8&psc=1&refRID=1NDZF7TSGY4EJ9THS28R): Libro fácil de seguir, cruce entre python y NLTK. Para iniciarse en el mundillo no está mal.\n"]},{"metadata":{"id":"s95S2Qj2-xfp","colab_type":"text"},"cell_type":"markdown","source":["### Twitter\n","\n","Twitter es fantástico. Twitter es terrible. Bueno, Twitter para NLP es una bendición. Podéis usarlo para aprender a generar twits como Donald Trump y mandárselos a vuestro peor enemigo o podeis aprender a generar frases célebres como **\"No todo lo que leemos es cierto. Platon 350 BC.\"** Podemos extraer los mejores insultos, ahora que es temporada de mundial de futbol, o leer los pensamientos de gente que nos guste y conocer sus razonamientos.\n","\n","Twitter es magnífico porque también podéis seguir a gente interesante en vuestro campo. Yo intentaré hacer una lista, para aquellos que os interese, de igual forma iré haciendo una lista de posts y contenidos de internet que parezcan interesantes.\n","\n","<div align=\"center\">\n","![](https://i.forbesimg.com/media/lists/companies/twitter_416x416.jpg)\n","</div>\n","\n","*   [Sebastian Ruder](https://twitter.com/seb_ruder)\n","*   [Jeremy Howard](https://twitter.com/jeremyphoward) More generalistic\n","*   [Chris Manning](https://twitter.com/chrmanning)\n","*   [Yoav Goldbert](https://twitter.com/yoavgo)\n","*   [Sam Bowman](https://twitter.com/sleepinyourhat)\n","*   [Graham Neubig](https://twitter.com/gneubig)\n","*   [Thang Luong](https://twitter.com/lmthang)\n","*   [Chris Olah](https://twitter.com/ch402)\n","*   [Francois Chollet](https://twitter.com/fchollet)\n","*   ...  y si queréis más,  ya los añadiré\n"]},{"metadata":{"id":"oTq6mjm979fz","colab_type":"text"},"cell_type":"markdown","source":["#### El siguiente snippet de código contiene las librerías más recurrentes a lo largo del curso"]},{"metadata":{"id":"7-t-1upe79ft","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":384},"outputId":"ba2acf44-8e55-447d-e0c1-6353a3200ae6","executionInfo":{"status":"error","timestamp":1528537554882,"user_tz":-120,"elapsed":2679,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["import keras\n","from sklearn.feature_extraction.text import CountVectorizer\n","import spacy\n","import numpy as np\n","\n","nlp_en = spacy.load('en_core_web_sm')\n","nlp_es = spacy.load('es_core_news_sm')\n","\n","'Si hemos llegado hasta aqui, vamos bien en el curso :D'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-1-9e910c78e867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: No module named spacy","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"ngcF9-D5iZlb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1328},"outputId":"3e31928d-c2e3-4efb-8643-b1e61e036357","executionInfo":{"status":"ok","timestamp":1530208883551,"user_tz":-120,"elapsed":310990,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["!pip install spacy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting spacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/31/e60f88751e48851b002f78a35221d12300783d5a43d4ef12fbf10cca96c3/spacy-2.0.11.tar.gz (17.6MB)\n","\u001b[K    100% |████████████████████████████████| 17.6MB 1.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from spacy) (1.14.5)\n","Collecting murmurhash<0.29,>=0.28 (from spacy)\n","  Downloading https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n","Collecting cymem<1.32,>=1.30 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/a2/76d5031797be8a7881537f1dd658142f993f6a18f0e946caa8ee20b7a2fa/cymem-1.31.2-cp27-cp27mu-manylinux1_x86_64.whl (66kB)\n","\u001b[K    100% |████████████████████████████████| 71kB 9.0MB/s \n","\u001b[?25hCollecting preshed<2.0.0,>=1.0.0 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/ac/7c17b1fd54b60972785b646d37da2826311cca70842c011c4ff84fbe95e0/preshed-1.0.0.tar.gz (89kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 24.8MB/s \n","\u001b[?25hCollecting thinc<6.11.0,>=6.10.1 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/fd/e9f36081e6f53699943381858848f3b4d759e0dd03c43b98807dde34c252/thinc-6.10.2.tar.gz (1.2MB)\n","\u001b[K    100% |████████████████████████████████| 1.2MB 15.1MB/s \n","\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Collecting pathlib (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 19.0MB/s \n","\u001b[?25hCollecting ujson>=1.35 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n","\u001b[K    100% |████████████████████████████████| 194kB 26.4MB/s \n","\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n","\u001b[K    100% |████████████████████████████████| 153kB 26.9MB/s \n","\u001b[?25hCollecting regex==2017.4.5 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n","\u001b[K    100% |████████████████████████████████| 604kB 22.3MB/s \n","\u001b[?25hCollecting wrapt (from thinc<6.11.0,>=6.10.1->spacy)\n","  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n","Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.1->spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 18.8MB/s \n","\u001b[?25hCollecting cytoolz<0.9,>=0.8 (from thinc<6.11.0,>=6.10.1->spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/e6/ccc124714dcc1bd511e64ddafb4d5d20ada2533b92e3173a4cf09e0d0831/cytoolz-0.8.2.tar.gz (386kB)\n","\u001b[K    100% |████████████████████████████████| 389kB 25.3MB/s \n","\u001b[?25hRequirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.11.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python2.7/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.1.0)\n","Collecting msgpack-python (from thinc<6.11.0,>=6.10.1->spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/20/6eca772d1a5830336f84aca1d8198e5a3f4715cd1c7fc36d3cc7f7185091/msgpack-python-0.5.6.tar.gz (138kB)\n","\u001b[K    100% |████████████████████████████████| 143kB 26.2MB/s \n","\u001b[?25hCollecting msgpack-numpy==0.4.1 (from thinc<6.11.0,>=6.10.1->spacy)\n","  Downloading https://files.pythonhosted.org/packages/2e/43/393e30e2768b0357541ac95891f96b80ccc4d517e0dd2fa3042fc8926538/msgpack_numpy-0.4.1-py2.py3-none-any.whl\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy) (0.9.0)\n","Building wheels for collected packages: spacy, murmurhash, preshed, thinc, pathlib, ujson, dill, regex, wrapt, cytoolz, msgpack-python\n","  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/"],"name":"stdout"},{"output_type":"stream","text":["\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/fb/00/28/75c85d5135e7d9a100639137d1847d41e914ed16c962d467e4\n","  Running setup.py bdist_wheel for murmurhash ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n","  Running setup.py bdist_wheel for preshed ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/8f/85/06/2d132fb649a6bbcab22487e4147880a55b0dd0f4b18fdfd6b5\n","  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d8/5c/3e/9acf5d9974fb1c9e7b467563ea5429c9325f67306e93147961\n","  Running setup.py bdist_wheel for pathlib ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171\n","  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n","  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n","  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n","  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n","  Running setup.py bdist_wheel for cytoolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f8/b1/86/c92e4d36b690208fff8471711b85eaa6bc6d19860a86199a09\n","  Running setup.py bdist_wheel for msgpack-python ... \u001b[?25l"],"name":"stdout"},{"output_type":"stream","text":["-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d5/de/86/7fa56fda12511be47ea0808f3502bc879df4e63ab168ec0406\n","Successfully built spacy murmurhash preshed thinc pathlib ujson dill regex wrapt cytoolz msgpack-python\n","Installing collected packages: murmurhash, cymem, preshed, wrapt, tqdm, cytoolz, plac, dill, pathlib, msgpack-python, msgpack-numpy, thinc, ujson, regex, spacy\n","Successfully installed cymem-1.31.2 cytoolz-0.8.2 dill-0.2.8.2 msgpack-numpy-0.4.1 msgpack-python-0.5.6 murmurhash-0.28.0 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 regex-2017.4.5 spacy-2.0.11 thinc-6.10.2 tqdm-4.23.4 ujson-1.35 wrapt-1.10.11\n"],"name":"stdout"}]},{"metadata":{"id":"r1Db69ox79f0","colab_type":"text"},"cell_type":"markdown","source":["# Que hacemos aquí? O que es NLP.\n","\n","Natural Language Processing. Es una disciplina dentro del enorme campo de la IA. No es Big Data. No es Deep Learning. Ni tan siquiera es Machine Learning. NLP es procesado de texto. Es como conseguir que un ordenador entienda lenguaje natural. Y el lenguaje natural es muy distinto al keyword matching de turno. NLP usa técnicas de ML y obviamente Deep Learning para conseguir su objetivo.\n","\n","Conseguir que las máquinas entiendan lenguaje natural al nivel humano es una tarea de muchos años de historia, que ha pasado por diferentes paradigmas, como todos los campos de la IA. Reglas, Statistics (Bayes), y ahora Deep Learning.\n","\n","NLP es el hijo entre IA y Lingüística, aunque ahora está de moda olvidar los de linguística, siempre vuelven.\n","\n","Esta no es la primera vez que alguien se cree que los linguistas son prescindibles.\n","\n","<div align=\"center\">\n","![](https://izquotes.com/quotes-pictures/quote-every-time-i-fire-a-linguist-the-performance-of-our-speech-recognition-system-goes-up-fred-jelinek-240586.jpg)\n","</div>\n","\n","## Que tareas podemos resolver?\n","\n","En el siguiente esquema, podemos ver un conjunto de tareas, no de NLP, sino únicamente de un asistente. A lo largo del curso, veremos unas cuántas de estas piezas.\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/xjkYHR7.png =600x)\n","</div>\n","    \n","## Y como llevamos esto?\n","\n","Pues en algunos campos dentro del NLP muy bien, otros... no tanto.\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/oR4Oeoh.png)\n","</div>\n","\n","Y cuáles son los problemas más interesantes en resolver? Pues los que tenemos más lejos, obviamente. Excepto Machine Translation, que tiene un potencial enorme."]},{"metadata":{"id":"U-rsHUyl79f1","colab_type":"text"},"cell_type":"markdown","source":["# ¿Y por qué es importante NLP?\n","\n","<div align=\"center\">\n","![](http://telecoms.com/wp-content/blogs.dir/1/files/2017/06/NLP-in-mobile.png =500x)\n","</div>\n","\n","En resumen, que el mundo se dirige hacia interficies de conversación, y conversación es NLP. Cortana, Alexa, Siri, Google Assistant, todas las grandes empresas estan trabajando en ello, aunque no estén aún a un nivel espectacular."]},{"metadata":{"id":"3-G90VmZ79f1","colab_type":"text"},"cell_type":"markdown","source":["## ¿Dónde estamos ahora mismo?\n","\n","<div align=\"center\">\n","![](http://cdn.smosh.com/wp-content/uploads/ftpuploads/bloguploads/1213/smartass-siri-kill-tomorrow.jpg)\n","</div>\n","\n","¿Que no os lo creéis?\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/Zt4IIgz.png)\n","</div>\n","\n","De hace 1 mes.\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/AKUbK2o.png)\n"," </div>"]},{"metadata":{"id":"MUd6wpQaIZhN","colab_type":"text"},"cell_type":"markdown","source":["# Vamos a ejemplos más prácticos.\n","\n","En las anteriores sesiones visteis Deep Learning, y seguramente bastante relacionado con visión. En lenguaje, los algoritmos aprendidos en Machine Learning y Deep Learning pasan a formar parte de nuestra toolbox, son simplemente herramientas para nuestro uso y disfrute. El objetivo no es aplicar por aplicar el algoritmo, sino que será siempre intentar resolver un problema real, con el que nos enfrentaremos a datos ¨reales¨. Seguramente ya habréis trabajado con datos de todo tipo, y estareis de acuerdo que los peores de todos son campos textuales.\n","\n","> ¿Dónde vive usted? (escoja la respuesta correcta)\n","    1. barcelona\n","    2. Barcelona\n","    3. barceLona\n","    4. BCN\n","    \n","Todos hemos pasado por fantásticas fases de limpiar datos en tablas sql o similares. Bueno, pues además, el lenguaje no es sólo el nuestro o el inglés, de hecho, en inglés es todo más fácil. мы все знаем, что английский проще, verdad? A nivel de ingeniería hay que acordarse de los encodings. Eso que se ve algún día en base de datos, lo pones y te olvidas. ¿Qué pasa si desarrollamos un buscador, por ejemplo, y nos entra una búsqueda en un idioma que no está preparado? ¿Cómo sabemos que esa búsqueda no está en el idioma que esperábamos? \n","\n","El lenguaje es altamente complejo, tenemos sinónimos, hiperónimos e hypónimos, mejor aún, decimos cosas que significan otras, decimos cosas que en un contexto significan una cosa y en otro, otra. En fin, que a lo largo de muchos años hemos desarrollado el lenguaje y lo hemos complicado tanto como hemos necesitado. Y para aquellos que nos dedicamos a intentar hacer que un ordenador aprenda lenguaje natural, la verdad, nos sobran unos 50.000 años de aprendizaje y desarrollo.\n","\n","Visto que esto del NLP es fácil, nosotros nos centraremos en el uso de técnicas ya desarrolladas para la obtención de features. Es decir, no reinventaremos la rueda. Usaremos librerías del ecosistema de python, que son magníficas, y reusaremos aquellas librerías que ya habeis conocido en ML y DL, así que no os tenéis que preocupar más que por conocer las nuevas librerías que nos proporcionarán features de texto.\n","\n","**NO** veremos nada de speech recognition en el curso. Asumiremos que todo el texto está escrito. Algunas veces estará bien escrito, otras no tanto.\n","\n","Haremos y veremos como solucionar problemas que ya habeis visto con anterioridad, pero no os habíais dado cuenta. ¿Podemos hace un clasificador de un gato o un perro? Podemos.\n","\n","¿Y un clasificador que te dé animales? También lo podemos hacer. Básicamente, podemos hacer las mismas cosas que en visión, pero con texto. ¿Si? ¿Seguro? Bueno...\n","\n","# Limitaciones o diferencias con visión\n","\n","Esto que comentaremos aquí son temas candentes en el research o, más bien aún, no hay grandes soluciones para tales problemáticas. La verdad es que en imagen, lo que vemos es lo que es. ¿Cierto? Bueno, tampoco. Pero no es tan obvio. \n","\n","[One Pixel Attacks](https://www.youtube.com/watch?v=SA4YEAWVpbk): este link es un vídeo de dos minutos de un canal super interesante de youtube. Os dejo una imagen para no tener que ir al vídeo ahora.\n","\n","<div align=\"center\">\n","![](https://pbs.twimg.com/media/DWvpECcU0AA49_X.jpg =400x) \n","</div>\n","\n","Está claro que todos, tanto lenguaje como visión, no son 100% hacking proof, es decir, nos pueden romper el sistema. En el caso de visión, una imagen mal tomada o modificada, podría hacer el efecto. El problema es que en el caso del lenguaje no hay que ir tan lejos. Basta con usar sarcasmo. O incluso memes.\n","\n","<div align=\"center\">\n","![](https://media.makeameme.org/created/si-claro-lo.jpg =400x)\n","</div>\n","\n","Cuando vemos este meme se entiende lo que digo, ¿verdad? O no. Seguramente es todo lo contrario. Pues ahí hay uno de los grandes problemas del lenguaje, no todo lo que se dice es literal. Y eso es muy complicado de hacérselo entender a un ordenador. Hay muchos estudios dedicados a esto, pero aún no estamos ahí.\n","\n","Un último ejemplo, en este caso de un tipo de problema que, aunque no trataremos en el curso, es un problema trabajado en NLP,  se llama *coreference resolution*,  y se trata de saber, cuando usamos un pronombre, a qué nombre nos referiamos. Por ejemplo: \n","> **The trophy didn’t fit on the shelf because it was too big.**\n","\n","¿Qué era muy grande? ¿El trofeo? ¿La estantería? Para nosotros es obvio, pero para resolver esto con algoritmos no es tán fácil.\n","\n","\n","\n","\n","\n","\n","    "]},{"metadata":{"id":"_FqeO2P8nLUS","colab_type":"text"},"cell_type":"markdown","source":["## Pipeline de NLP\n","\n","Antes de ir al lío, hay un diagrama que me gustariía que tuviérais presente.\n","\n","<div align=\"center\">\n","![](https://dzone.com/storage/temp/3307020-ml-pipeline.png)\n","</div>\n","\n","Seguramente habeis visto el pipeline resolviendo algun problema de Deep Learning o Machine Learning, pero es que en NLP es exactamente igual. En el caso de Deep Learning, el feature extractor forma parte del algoritmo, pero no hay nada que no hayáis visto hasta ahora. Excepto que en el caso de NLP, en el punto de feature extractor, lo podríamos cambiar por data - preprocessing, que es lo que veremos hoy.\n","\n","Es importante que cuando implementéis una solución para Lenguaje, o para cualquier tipo de solución en NLP, uséis el mismo pipeline tanto para training, como para inference (prediction), con lo que el data-preprocessing forme tambien parte del pipeline de inference.\n","\n","**Ahora que ya hemos planteado que es esto del NLP, y qué dificultades nos vamos a encontrar, vamos allá!**"]},{"metadata":{"id":"jeLxrq4toWR6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}