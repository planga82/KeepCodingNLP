{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FeatRep.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python [conda env:MythNLP]","language":"python","name":"conda-env-MythNLP-py"}},"cells":[{"metadata":{"id":"mZ6kAyglcwps","colab_type":"text"},"cell_type":"markdown","source":["# Text Representation\n","## BoW and n-grams\n","\n","Empezaremos por la representación más clave en NLP, que es la Bolsa de palabras.\n","\n","La bolsa de palabras es extremadamente fácil de crear y extremadamente usada como input vector en muchísimos algoritmos de lenguaje. Nos permitirá recuperar numpy y ver la primera interacción con sklearn.\n","\n","## ¿En qué consiste?\n","\n","Consiste en representar un input textual como un vector. ¿Qué valor tendrá cada casilla de un vector? Fácil, primero generaremos un vocabulario de nuestro train data. Luego, asignaremos un entero a cada palabra y ese entero será el índice de este vector. Finalmente, por cada frase, asignaremos en cada posición del vector el número de veces que aparece en frases.\n","\n","![](https://i.imgur.com/EPG2zSK.jpg)\n","\n","Vamos a ello paso a paso."]},{"metadata":{"id":"C1_36_JLcwp1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pZtG85sTcwqC","colab_type":"text"},"cell_type":"markdown","source":["### ¿Usaremos todas las palabras?\n","\n","No siempre. Ver stop-words.\n","\n","¿Alguna cosilla más en el vocabulary? UNK, PAD tokens."]},{"metadata":{"id":"AjvGwGCOcwqD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ARZKqbg4cwqI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"c5b63bc0-92ca-41b4-ffd6-1bf47a7d11b5"},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'muy': 10, 'tienen': 9, 'el': 13, 'mucho': 21, 'frases': 19, 'que': 23, 'en': 1, 'emacs.': 24, 'english': 3, 'vs': 26, 'esta': 5, 'frase': 0, 'no': 6, 'documentos': 8, 'de': 18, 'conjunto': 17, 'sentence': 4, 'castellano': 2, 'estos': 7, 'mejor': 22, 'documento': 14, 'spaces': 27, 'Vim': 20, 'es': 15, 'tabs': 25, 'un': 16, 'sentido': 12, 'poco': 11}\n"],"name":"stdout"}]},{"metadata":{"id":"KLKO-oXgcwqT","colab_type":"text"},"cell_type":"markdown","source":["## Introducción de numpy en el curso.\n"]},{"metadata":{"id":"fQOv2emUcwqU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"79gwPfezcwqb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wz82TBuxcwqh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ISK5p1s8cwql","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"seXE7qFGcwqo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LM-d-tkdcwqt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"EJy88m7Fcwqx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"qI8eLUQ0cwq1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Qrxt8r5cwq4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0yeTc90jcwq6","colab_type":"text"},"cell_type":"markdown","source":["Ahora que ya hemos visto lo rápido que es preparar un modelo de Bolsa de palabras, habría que hacer un pequeno análisis de los problemas que puede tener.\n","\n","Ver Bow Issues en OneNote.\n","\n","¿Cuán grande puede ser un vocabulario?\n","\n","El español tiene 100.000 palabras aproximadamente, sin contar con conjugaciones verbales.\n","\n","<div align=\"center\">\n","![](https://i.imgur.com/a29W4JC.png)\n","</div>"]},{"metadata":{"id":"5HAiBQZz1CJp","colab_type":"text"},"cell_type":"markdown","source":["# N-grams\n","\n","Una de las posibles soluciones al modelo de Bolsa de Palabras es una ampliación bastante natural. En lugar de coger palabras sueltas, cogemos n-palabras consecutivas. A esto lo denominamos n-grams.\n","\n","Los bigrams, y trigrams son modelos de features bastante usados, hasta 5-grams, y no son de uso exclusivo. Es decir, podemos concatenar el input de una frase usando [unigram_vector, bigram_vector, trigram_vector].\n","\n","p.e.:\n","\n","> ¿Qué puedo hacer?\n","\n","> Me dijo que quería hacer algo.\n","\n","En un problema de clasificación, quizás es importante saber órdenes, aunque sean parciales.\n","\n","La siguiente quote proviene de Google, cuando hicieron una release de los n-grams que calcularon en la web.\n","\n","> \n","We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 times.\n","\n","\n","\n","Ahora veremos como se implementa un simple modelo de n-grams."]},{"metadata":{"id":"ktSqGmOqcwq7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"GoBdtG4Jcwq9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8l_7VRU3cwrA","colab_type":"text"},"cell_type":"markdown","source":["En este punto ya tenemos unas features básicas para poder realizar tareas de clasificación de textos con un clasificador como bayes.\n","\n","Solo nos quedaría mapear esto a un numpy array. En estos ejemplos todo es magnífico. Pero, ¿y en la realidad? ¡Vectores enormes! ¡Explosión de features! ¿Qué pasa con los vocabularios muy extensos?"]},{"metadata":{"id":"olw0KvQ_cwrD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"d_Ek0hFzcwrE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Q2Nxq1vcwrH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"68f2cd03-f6ff-42fe-9431-e6555a572f56"},"cell_type":"code","source":["print('Hay', len(docs), 'frases')\n","print('Hemos generado', len(feature_map), 'features')\n","print('Solo teniamos', len(set([token for doc in docs for token in doc.split(' ')])), 'palabras unicas')\n","print('Comprobacion con ngrams', len(ngrams[1])-2, 'quitando <SOS> y <EOS>')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Hay 9 frases\n","Hemos generado 82 features\n","Solo teniamos 30 palabras unicas\n","Comprobacion con ngrams 30 quitando <SOS> y <EOS>\n"],"name":"stdout"}]},{"metadata":{"id":"Guv_7UVEcwrK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"0lSQ_iWVcwrN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"OHlrAJstcwrP","colab_type":"text"},"cell_type":"markdown","source":["Se generan vectores enormes con muchos 0 y pocos 1. Más adelante veremos cómo dejar de tener vectores tan grandes. Hay dos tipos de representación en vectores: sparse, que es la que tenemos aquí, y dense, que la veremos en PCA/SVD o word embeddings.  "]},{"metadata":{"id":"Lg-F0zQlcwrP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fx0J_orXcwrU","colab_type":"text"},"cell_type":"markdown","source":[" <div><font color=\"red\">\n","\n","NLP tiene un gran problema que acabamos de ver. El vocabulario es ~infinito.</font>\n","\n","El vocabulario se puede generar vocabulario usando tu train data, tambien se puede generar vocabulario con las palabras más usadas en un idioma. Es complicadísimo generar un vocabulario general para una tarea que contenga NLP.\n","</div>\n","\n","De hecho, a lo largo de las sesiones, veremos que pasa con todas aquellas palabras Out Of Vocabulary (OOV). Es importante que tengamos en cuenta que esto nos va a ocurrir, y como modelamos nuestros problemas para que su afectación sea mínima, o nos permita generalizar de forma óptima.\n","\n","\n"," *Es MUY importante en nuestros pipelines de NLP intentar controlar el vocabulario de alguna manera.*\n","\n","Hay varias maneras de hacerlo e iremos viendolas a lo largo de las sesiones. Ya hemos visto una (distancia de edición) para intentar corregir errores ortográficos. En problemas de clasificación con metodos de representación más clásicos, usaremos smoothing. En deep learning hay la tendencia de \"unkificarlo todo\", es decir todos aquellos tokens que desconocemos, tratarlos con un token especial < UNK >\n","\n","A continuación veremos lo mismo de antes con la libreria sklearn, muy recomendable tirar de ella tanto para feature extraction como para clasificación."]},{"metadata":{"id":"5Le5PDVxcwrV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AaH6gipPcwrY","colab_type":"text"},"cell_type":"markdown","source":["http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"]},{"metadata":{"id":"RzxcULhDcwrZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xmyWI_L4cwrc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"rWXee2-Qcwrf","colab_type":"text"},"cell_type":"markdown","source":["Es muy fácil de usar, bastante eficiente y 100% integrable con otras librerías, ya que devuelve numpy arrays, que son la base para muchas otras librerías.\n","\n","En caso de que el vocabulario sea muy grande, en lugar de utilizar CountVectorizer usaríamos http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer .\n","\n","Es más eficiente a nivel de memoria, pero con el drawback de que no podríamos sacar el inverso de las features que calcula. De todas formas, pronto veremos que los one hot vectors se usan cada vez menos. \n","\n","Normalmente, todos estas funciones como el CountVectorizer, o cualquier algoritmo que queramos implementar, debería permitir-nos poder usar tanto palabras, como carácteres como features. Aquí ya podemos verlo, y lo veremos mucho mejor en la siguiente sesión!"]}]}