{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["XL1NQjYqkXjL","94Lc8jzsJA5J","nT68AKpKxKWW","mz8irJdSxKWz","qFgDxMc9JSr-","xujVkl70xKXO","pLnTOi2ixKXj","sUbRwAzyxKXt"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sPJSlwGfMNLZ","colab_type":"text"},"cell_type":"markdown","source":["# Arquitecturas avanzadas de NLP. Sequence 2 Sequence\n","\n","[Uno de los mejores resources](https://distill.pub/2016/augmented-rnns/)\n","\n","Hoy/ahora, veremos una de las arquitecturas más usadas, para un tipo de problema muy concreto, pero muy extendido. Si os acordais de Language Modeling, generábamos secuencias, con una palabra o carácter de semilla, pero podía ir a cualquier parte la secuencia. Con Sequence 2 Sequence (Seq2Seq - S2S), lo que queremos es condicionar el output de nuestra arquitectura a todo un contexto.\n","\n","![Seq2Seq](https://cdn-images-1.medium.com/max/1600/1*_6-EVV3RJXD5KDjdnxztzg@2x.png)\n","\n","Esta arquitectura apareció por primera vez en 2014, [paper](https://www.aclweb.org/anthology/D14-1179), y aunque tuvo mucha repercusión, no fue hasta que en 2015... lo vemos despues!\n","\n","Estas arquitecturas, ya en 2014 mostraron gran potencial, y se confirmó cuando empresas como Google, decidieron cambiar productos tan importantes como el Google Translate a arquitecturas similares a esta. Os dejó un blog [post](https://codesachin.wordpress.com/2017/01/18/understanding-the-new-google-translate/) para que veais, y el [paper](https://arxiv.org/pdf/1609.08144.pdf) original dónde se explica el funcionamiento de la arquitectura. Otra aplicación bastante interesante, es la de generar respuestas a correos electrónicos de forma automática [info aquí](https://ai.googleblog.com/2015/11/computer-respond-to-this-email.html).\n","\n","### Vale bien, pero que hace esto?\n","\n","El dibujo de arriba, si seguis el timeline (time-steps), queda bastante claro. Primero codificamos un mensaje (steps-1-4), luego inicializamos el decoder (flecha entre 4 y 5), y empezamos la decodificación (5-7). \n","\n","Durante el entreno, se pasan dos inputs distintos, el input del encoder, y el del decoder. El input del encoder aquí sería\n","\n","\n"]},{"metadata":{"id":"p_Z4-b6BlpY8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"9232ee5e-adc9-4e51-d772-20fda9611ef4","executionInfo":{"status":"ok","timestamp":1531215410359,"user_tz":-120,"elapsed":790,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["input_encoder = [\"<SOS>\", \"how\", \"are\", \"you\", \"?\", \"<EOS>\"]\n","input_decoder = [\"<SOS>\", \"I\", \"am\", \"good\", \"<EOS>\"]\n","target_decoder = input_decoder[1:]\n","target_decoder"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I', 'am', 'good', '<EOS>']"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"GvA_6Qh1lr3T","colab_type":"text"},"cell_type":"markdown","source":["Fijaros que el encoder no tiene un target, sino que calcularemos la loss en el decoder, y pasaremos el gradiente hacía atrás desde el decoder, hasta el primer step del encoder.\n","\n","Del dibujo de arriba, ya lo habeis visto absolutamente todo. Las celdas azules verdes y azules son...? Normalmente son celdas RNN, ya sean vanilla RNN, LSTMs o GRUs (que aunque no las hemos introducido en el curso, son una variamente algo más \"barata\" que las LSTMs).\n","\n","La única flecha que os debería preocupar es la de verde a azul. En esa parte estamos copiando el último estado de nuestra encoder-RNN, y usandolo como estado inicial en el decoder-RNN. Si os acordáis del dibujo, de las LSTMs, básicamente queremos h_t, cuando t = T, es decir en su último estado.\n","\n","![](https://i.imgur.com/nE53oh1.png)\n","\n","### Keras tip\n","\n","![](https://i.imgur.com/V9Pkm8d.png)\n","\n","Keras nos permite obtener el estado de una LSTM, seteando return_state a True.\n","\n","Esto modifica el output, devolviendonos 3 tensores, el output, el hidden state, i el cell state. En caso normal, el output i el hidden state, son lo mismo, pero esto puede ser modificado con el keyword return_sequences, que nos permite obtener una matriz con un output por timestep, el hidden state (del último estado) y el cell_state del último estado.\n","\n","Ahora que ya sabemos como recoger el hidden state, ya podemos ir a implementar un Seq2Seq en Keras.\n","\n","No implementaremos nada en numpy, porque ya tenemos casi todo implementado (almenos el forward pass) en sesiones pasadas, con lo que saltaremos a Keras directamente.\n"]},{"metadata":{"id":"XL1NQjYqkXjL","colab_type":"text"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"WXvp_fvsxKV9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"2613a192-8bd1-4e03-ce7f-fef61c106ffe","executionInfo":{"status":"ok","timestamp":1531215413132,"user_tz":-120,"elapsed":1594,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Embedding\n","from keras.preprocessing import sequence\n","from keras.callbacks import Callback\n","from keras.layers import TimeDistributed\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","import numpy as np\n","from random import shuffle, choice, sample\n","\n","import pprint as pp\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import pylab as pl\n","from IPython import display\n","\n","sns.set(color_codes=True)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","%matplotlib notebook"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"94Lc8jzsJA5J","colab_type":"text"},"cell_type":"markdown","source":["## Data"]},{"metadata":{"id":"08ZzfxETc0ql","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate_dummy_data():\n","    x = [ix for ix in range(100)]\n","    data = []\n","    for ix_source in range(3, 5):\n","        for ix_target in range(3, 5):\n","            for ix, _ in enumerate(x):\n","                data.append((x[ix:ix+ix_source], x[ix+ix_source:ix+ix_target*2]))\n","    return data\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"XU-BCbjdd7F7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["dummy_data = generate_dummy_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iWVoNQPqxKWQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["shuffle(dummy_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nT68AKpKxKWW","colab_type":"text"},"cell_type":"markdown","source":["## Data Preprocess"]},{"metadata":{"id":"iahClNecxKWY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vIHDdlVqopCG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"dGD-24qOxKWe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#vocabulary preparation\n","vocab = []\n","for inp, out in data_tr:\n","    vocab+=[w for w in inp]\n","    vocab+=[w for w in out]\n","vocab = list(set(vocab))\n","vocab.insert(0, '<PAD>')\n","vocab.append('<UNK>')\n","print(vocab)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y-PY3iZExKWm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["w2id = {w:i for i, w in enumerate(vocab)}\n","id2w = {w:i for i, w in w2id.items()}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xfULT57_xKWs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"mz8irJdSxKWz","colab_type":"text"},"cell_type":"markdown","source":["## Auxiliary Functions"]},{"metadata":{"id":"GY7ZXd-rxKW0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Sampletest(Callback):\n","    def on_epoch_end(self, epoch, logs):\n","        if epoch % 10 == 0  and epoch>0:\n","            nb_samples = 1\n","            data_t = sample(data_tr, nb_samples)\n","            data_test = []\n","            for inp, out in data_t:\n","                ind_enc_in = [w2id[w] if w in w2id else w2id['<UNK>'] for w in inp]\n","                ind_dec_in = [w2id[w] if w in w2id else w2id['<UNK>'] for w in out]\n","                data_test.append((ind_enc_in,ind_dec_in))\n","\n","            params = {\n","                'max_encoder_len': maxlen_source+2,\n","                'max_decoder_len': maxlen_source+2,\n","                'target_len': len(vocab)\n","                }\n","\n","            encoder_input_data = np.zeros(shape=(nb_samples, params['max_encoder_len'], params['target_len']))    \n","            decoder_input_data = np.zeros(shape=(nb_samples, params['max_decoder_len'], params['target_len']))\n","\n","            for i, (ei, di) in enumerate(data_test):\n","                for j, idx in enumerate(ei):\n","                    encoder_input_data[i, j, idx] = 1\n","                for j, idx_di in enumerate(di):\n","                    decoder_input_data[i, j, idx_di] = 1\n","\n","            result = self.model.predict([encoder_input_data, decoder_input_data])\n","            for r, original in zip(result, data_t):\n","                original_sentence = original[0]\n","                idx = np.argmax(r, axis=1)\n","                print(idx)\n","                repr_out = []\n","                for ix in idx:\n","                    token = id2w[ix]\n","                    if token == '<EOS>':\n","                        break\n","                    else:\n","                        repr_out.append(token)\n","                #print('   '*40, end='\\r')\n","                print('Test Sample epoch({}): {} ====> {}'.format(epoch, original_sentence, \" \".join(repr_out)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CDsE61T-xKW5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class HistoryDisplay(Callback):\n","    \n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.accs = []\n","        self.epochs = []\n","        self.fig, self.ax = plt.subplots()\n","        #plt.show()\n","        \n","        plt.ion()\n","        self.fig.show()\n","        self.fig.canvas.draw()\n","    \n","    def on_epoch_end(self, epoch, logs):\n","        self.epochs.append(epoch)\n","        self.losses.append(logs['loss'])\n","        self.accs.append(logs['acc'])\n","        if epoch % 3 == 0:\n","            \n","            self.ax.clear()\n","            self.ax.plot(self.epochs, self.accs, 'g--', label='acc')\n","            self.ax.plot(self.epochs, self.losses, 'b--', label='loss')\n","            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n","            #display.clear_output(wait=True)\n","            #display.display(pl.gcf())\n","            self.fig.canvas.draw()\n","            \n","            #plt.draw()\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"qFgDxMc9JSr-","colab_type":"text"},"cell_type":"markdown","source":["## Architecture definition"]},{"metadata":{"id":"Dz0BdGQGxKXD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Seq2Seq:\n","    def __init__(self, **kwargs):\n","        self.params = kwargs.pop('params', None)\n","    \n","    def compile_basic_seq2seq(self, params={}):\n","        \n","        \n","        return \"model\"\n","        \n","    def train(self, model, data, params={}):\n","        \n","        callbacks = self._get_callbacks()\n","        if 'shuffle' in params and params['shuffle']:\n","            shuffle(data)\n","        \n","        encoder_input_data = np.zeros(shape=(len(data), train_params['max_encoder_len'], train_params['target_len']))    \n","        decoder_input_data = np.zeros(shape=(len(data), train_params['max_decoder_len'], train_params['target_len']))\n","        decoder_target_data = np.zeros(shape=(len(data), train_params['max_decoder_len'], train_params['target_len']))\n","        for i, (ei, di,dt) in enumerate(data):\n","            for j, idx in enumerate(ei):\n","                encoder_input_data[i, j, idx] = 1\n","            for j, idx_di in enumerate(di):\n","                decoder_input_data[i, j, idx_di] = 1\n","            for j, idx_dt in enumerate(dt):\n","                decoder_target_data[i, j, idx_dt] = 1       \n","                \n","        model.fit(\"definir fit\")\n","            \n","    def predict(self, model, data, params={}):\n","        \n","        encoder_input_data = np.zeros(shape=(len(data), params['max_encoder_len'], params['target_len']))    \n","        decoder_input_data = np.zeros(shape=(len(data), params['max_decoder_len'], params['target_len']))\n","        \n","        for i, data_in in enumerate(data):\n","            if len(data_in) == 3:\n","                (ei, di, _) = data_in\n","            else:\n","                (ei, di) = data_in\n","            for j, idx in enumerate(ei):\n","                encoder_input_data[i, j, idx] = 1\n","            for j, idx_di in enumerate(di):\n","                decoder_input_data[i, j, idx_di] = 1\n","                \n","        return model.predict([encoder_input_data, decoder_input_data])\n","    \n","    def load(self, model_path='seq2seq.h5'):\n","        return load_model(model_path)\n","    \n","    def _get_callbacks(self, model_path='seq2seq.h5'):\n","        es = EarlyStopping(monitor='loss', patience=20, mode='auto', verbose=1)\n","        save_best = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only=True, save_weights_only=False, period=2)\n","        st = Sampletest()\n","        # hd = HistoryDisplay()\n","        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n","              patience=5, min_lr=0.0001, verbose=1)\n","        return [st, save_best, rlr]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xujVkl70xKXO","colab_type":"text"},"cell_type":"markdown","source":["## Compile model definition"]},{"metadata":{"id":"wbgoZhQIxKXQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"PkdG6LOxxKXc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pLnTOi2ixKXj","colab_type":"text"},"cell_type":"markdown","source":["## Seq2Seq Train"]},{"metadata":{"id":"CjbIHENSxKXn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"sUbRwAzyxKXt","colab_type":"text"},"cell_type":"markdown","source":["## Predict"]},{"metadata":{"id":"1WZ7Nd0MxKXu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"zRwH09lPxKXx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7tGij_2YxKX0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}