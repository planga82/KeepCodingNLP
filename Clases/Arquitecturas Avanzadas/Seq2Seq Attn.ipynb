{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq Attn.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["i5_oV-zCY-av","A8SYkTpHYqVo","702qFiEUxHZF","MmslbSP5xHZf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"INzqe5KoY6IN","colab_type":"text"},"cell_type":"markdown","source":["# Sequence to Sequence with Attention\n","\n","Los modelos encoder-decoder tienen un problema, si os fijasteis, y sino, ver foto, el contexto del encoder era el útlimo estado de nuestra red recurrente. Es decir, pretendemos que una frase, da igual si es corta, larga o lo que sea, pretendemos que quede en un vector de tamaño fijo, ya sea 64 o 1024. Obviamente, si usamos 1024 posiciones sería mejor, pero no dejamos de depender de un vector de tamaño fijo.\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","Es por eso, que en 2015, empezaron a salir, basados de nuevo en Computer Vision los llamádos módulos de atención. Estos módulos nos permiten que nuestras redes aprendan dónde hay información relevante en el encoder. Y muy importante, nos permiten ver a nosotros donde se estan fijando.\n","\n","![](https://i.imgur.com/JaSRu42.png)\n","\n","Estos modelos, son usados ya en cualquier sistema encoder-decoder, de hecho en los links que hemos visto anteriormente sobre el sistema de Google Translate, y cualquier otro sistema, usan ya estos sistemas. Aquí podeis ver un ejemplo aplicado a la traducción francés-inglés visto arriba.\n","\n","![](https://camo.githubusercontent.com/c54ad54bebb12b5b585ab666664e6d0e6002d894/68747470733a2f2f692e696d6775722e636f6d2f357936534376552e706e67 =600x)\n","\n","## Explain Attention Network\n","\n","Basicamente, el cambio que hacemos en esta arquitectura es el siguiente. En lugar de usar sólo el ultimo hidden_state de nuestro encoder, vamos a usarlos todos, y dejar que la red aprenda donde hay información relevante para producir el output. Es decir, en lugar de fijar-nos en lo último, o de fijar-nos en todo, ayudamos a nuestra arquitectura a fijar-se en aquello que es relevante.\n","\n","Ver ejemplo en [Attentional interfaces](https://distill.pub/2016/augmented-rnns/#attentional-interfaces)\n","\n","Hay varias maneras de calcular este nuevo contexto, nosotros implementaremos una en Keras (Luong's Attention, del segundo paper que os recomiendo aquí.) \n","\n","La forma de conseguir esto, no es distinta a nada que no hayamos hecho hasta la fecha, dot products, quizás una fully connected layer, y una softmax para generar una distribución que diga que partes son las más importantes.\n","\n","En estos dos papers, podeis leer en más detalle el funcionamiento de los módulos de atención.\n","\n","*   [Neural Machine Translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)\n","*   [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)\n","\n","Son papers algo complicados, pero muy influyentes en las tareas de NLP actuales.\n","\n","A continuación, haremos una simple implementación de un módulo de atención en numpy, y luego lo implementaremos en Keras modificando así nuestra arquitectura encoder-decoder (Sequence 2 Sequence)\n","\n"]},{"metadata":{"id":"_W8dD-y3Wz6M","colab_type":"text"},"cell_type":"markdown","source":["## Numpy Implementation"]},{"metadata":{"id":"YLsoPgUrKFjE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9eIf_sOUMzc-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def softmax(x, axis=0):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=axis) # only difference\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gyxSLSxoZoZu","colab_type":"text"},"cell_type":"markdown","source":["#### Alignment score\n","\n","![](https://i.imgur.com/pjJrL24.png =400x)\n","\n","#### Alignment vector\n","\n","![](https://i.imgur.com/QAOe4Qk.png =400x)\n","\n","#### Context vector\n","\n","![](https://i.imgur.com/4fjOKja.png =350x)\n","\n"]},{"metadata":{"id":"Z9nOSC7WKHIi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Eg1Y-5PyQ6jt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gNbKUJsAUjGD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"374cdfe7-9295-4887-dff1-5ff19e3ce6fc","executionInfo":{"status":"ok","timestamp":1530181000719,"user_tz":-120,"elapsed":1157,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 5)"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"w9CbShb9exUs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"i5_oV-zCY-av","colab_type":"text"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"W-ofFt27xHYu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"86aabd14-c53a-4588-bbf4-eccc89f64ad7","executionInfo":{"status":"ok","timestamp":1531216522013,"user_tz":-120,"elapsed":3025,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Embedding\n","from keras.layers import Merge, Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate, Average\n","from keras.layers import TimeDistributed\n","from keras.layers import Activation\n","from keras.activations import softmax\n","from keras.preprocessing import sequence\n","from keras.callbacks import Callback\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","import tensorflow as tf\n","import keras.backend as K\n","\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","\n","from random import shuffle, choice, sample\n","import time\n","\n","import pprint as pp\n","import pickle\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import pylab as pl\n","from IPython import display\n","\n","sns.set(color_codes=True)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","%matplotlib notebook"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"A8SYkTpHYqVo","colab_type":"text"},"cell_type":"markdown","source":["## Data"]},{"metadata":{"id":"L_wM95I6xHY4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["USE_EMBEDDINGS = True\n","SAMPLE_EVERY = 3\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yQD9aXDvxHY9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate_dummy_data():\n","    x = [ix for ix in range(100)]\n","    data = []\n","    for ix_source in range(3, 5):\n","        for ix_target in range(3, 5):\n","            for ix, _ in enumerate(x):\n","                data.append((x[ix:ix+ix_source], x[ix+ix_source:ix+ix_target*2]))\n","    return data\n","    \n","dummy_data = generate_dummy_data()\n","shuffle(dummy_data)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kKve_smSpdme","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[" # sentence preparation\n","data_tr = []\n","for i, (inp, out) in enumerate(dummy_data):\n","    # _in = inp.split()\n","    # _in = inp.split()\n","    \n","    inp.insert(0, '<SOS>')\n","    out.insert(0, '<SOS>')\n","    \n","    inp.append('<EOS>')\n","    out.append('<EOS>')\n","    \n","    data_tr.append((inp, out))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n2XDa9OLxHZA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"197626b5-2e3b-44c4-94a4-8eab3a81e001","executionInfo":{"status":"ok","timestamp":1531217785219,"user_tz":-120,"elapsed":592,"user":{"displayName":"David Torrejon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100674754889499666756"}}},"cell_type":"code","source":["maxlen_source = max([len(x) for x, _ in dummy_data])\n","maxlen_source"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"702qFiEUxHZF","colab_type":"text"},"cell_type":"markdown","source":["## Data Preparation"]},{"metadata":{"id":"_vFuAQ0sxHZR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#vocabulary preparation\n","vocab = []\n","for inp, out in data_tr:\n","    vocab+=[w for w in inp]\n","    vocab+=[w for w in out]\n","vocab = list(set(vocab))\n","vocab.insert(0, '<PAD>')\n","vocab.append('<UNK>')\n","print(vocab)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R3KXFsqGxHZV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["w2id = {w:i for i, w in enumerate(vocab)}\n","id2w = {w:i for i, w in w2id.items()}\n","#pp.pprint(id2w)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gbymh3YzxHZZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["data_train = []\n","for inp, out in data_tr:\n","    ind_enc_in = [w2id[w] if w in w2id else w2id['<UNK>'] for w in inp]\n","    ind_dec_in = [w2id[w] if w in w2id else w2id['<UNK>'] for w in out]\n","    ind_dec_out = [w2id[w] if w in w2id else w2id['<UNK>'] for w in out[1:]]\n","    data_train.append((ind_enc_in, ind_dec_in, ind_dec_out))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MmslbSP5xHZf","colab_type":"text"},"cell_type":"markdown","source":["## Auxiliary functions"]},{"metadata":{"id":"djJKkTb90eUj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def sample_pred(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    # print('sample pred: ',probas.shape) # apply here LM\n","    return np.argmax(probas)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yRHc24apxHZi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Sampletest(Callback):\n","    def on_epoch_end(self, epoch, logs):\n","        if epoch % 5 == 0  and epoch>0:\n","            nb_samples = 1\n","            data_t = sample(data_tr, nb_samples)\n","            data_test = []\n","            for inp, out in data_t:\n","                ind_enc_in = [w2id[w] if w in w2id else w2id['<UNK>'] for w in inp]\n","                ind_dec_in = [w2id['<SOS>']] # w2id[w] if w in w2id else w2id['<UNK>'] for w in out.split(' ')\n","                data_test.append((ind_enc_in,ind_dec_in))\n","\n","            params = {\n","                'max_encoder_len': maxlen_source+2,\n","                'max_decoder_len': maxlen_source+2,\n","                'target_len': len(vocab),\n","                'use_embeddings': True\n","                }\n","            if 'use_embeddings' in params and params['use_embeddings']:\n","                encoder_input_data = np.zeros(shape=(nb_samples, train_params['max_encoder_len']))    \n","                decoder_input_data = np.zeros(shape=(nb_samples, train_params['max_decoder_len']))\n","                for i, (ei, di) in enumerate(data_test):\n","                    for j, idx in enumerate(ei):\n","                        encoder_input_data[i, j] = idx\n","                    for j, idx_di in enumerate(di):\n","                        decoder_input_data[i, j] = idx_di        \n","            else:\n","                encoder_input_data = np.zeros(shape=(nb_samples, params['max_encoder_len'], params['target_len']))    \n","                decoder_input_data = np.zeros(shape=(nb_samples, params['max_decoder_len'], params['target_len']))\n","\n","                for i, (ei, di) in enumerate(data_test):\n","                    for j, idx in enumerate(ei):\n","                        encoder_input_data[i, j, idx] = 1\n","                    for j, idx_di in enumerate(di):\n","                        decoder_input_data[i, j, idx_di] = 1\n","            temperature = choice([0.1, 0.3, 1.5])\n","            for i in range(1, params['max_decoder_len']):\n","                output = self.model.predict([encoder_input_data, decoder_input_data]) #.argmax(axis=2)\n","                # print(output.shape)\n","                output_t = np.apply_along_axis(sample_pred, 2, output, temperature=temperature)\n","                # print(output_t.shape)\n","                decoder_input_data[:,i] = output_t[:,i]\n","                \n","            result = decoder_input_data\n","            for r, original in zip(result, data_t):\n","                \n","                sentence = original\n","                repr_out = []\n","                for ix in r:\n","                    token = id2w[ix]\n","                    if token == '<EOS>':\n","                        break\n","                    else:\n","                        repr_out.append(token)\n","            \n","                print('Test Sample epoch({}): {} ====> {}'.format(epoch, sentence, repr_out[1:]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fq5cJ7I6xHZl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class HistoryDisplay(Callback):\n","    \n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.accs = []\n","        self.epochs = []\n","        self.fig, self.ax = plt.subplots()\n","        #plt.show()\n","        \n","        plt.ion()\n","        self.fig.show()\n","        self.fig.canvas.draw()\n","    \n","    def on_epoch_end(self, epoch, logs):\n","        self.epochs.append(epoch)\n","        self.losses.append(logs['loss'])\n","        self.accs.append(logs['acc'])\n","        if epoch % PLOT_EVERY == 0:\n","            \n","            self.ax.clear()\n","            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n","            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n","            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n","            #display.clear_output(wait=True)\n","            #display.display(pl.gcf())\n","            self.fig.canvas.draw()\n","            \n","            #plt.draw()\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"BJLyYqlGYs_H","colab_type":"text"},"cell_type":"markdown","source":["## Architecture Defintion"]},{"metadata":{"id":"gG0dh3kUawcq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from keras.engine.topology import Layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dUXHWc5VxHZ0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Seq2Seq:\n","    def __init__(self, **kwargs):\n","        self.params = kwargs.pop('params', None)\n","    \n","    def compile_basic_seq2seq(self, params={}):\n","        \n","        \n","        \n","        \n","        \n","        return \"model\"\n","        \n","    def train(self, model, data, params={}):\n","        \n","        callbacks = self._get_callbacks()\n","        print(callbacks)\n","        if 'shuffle' in params and params['shuffle']:\n","            shuffle(data)\n","        encoder_input_data = np.zeros(shape=(len(data), params['max_encoder_len']))    \n","        decoder_input_data = np.zeros(shape=(len(data), params['max_decoder_len']))\n","        decoder_target_data = np.zeros(shape=(len(data), params['max_decoder_len'], params['target_len']))\n","        for i, (ei, di,dt) in enumerate(data):\n","            for j, idx in enumerate(ei):\n","                encoder_input_data[i, j] = idx\n","            for j, idx_di in enumerate(di):\n","                decoder_input_data[i, j] = idx_di\n","            for j in range(params['max_decoder_len']):      \n","                if j<len(dt):\n","                    decoder_target_data[i, j, dt[j]] = 1\n","                else:\n","                    decoder_target_data[i, j, 0] = 1 \n","        model.fit(\"definir fit\")\n","            \n","    def predict(self, model, data, params={}):        \n","\n","\n","        encoder_input_data = np.zeros(shape=(len(data), train_params['max_encoder_len']))    \n","        decoder_input_data = np.zeros(shape=(len(data), train_params['max_decoder_len']))\n","        for i, data_in in enumerate(data):\n","            if len(data_in) == 3:\n","                (ei, di, _) = data_in\n","            else:\n","                (ei, di) = data_in\n","            for j, idx_ei in enumerate(ei):\n","                encoder_input_data[i, j] = idx_ei\n","            for j, idx_di in enumerate(di):\n","                decoder_input_data[i, j] = idx_di\n","        for i in range(1, params['max_decoder_len']):\n","            output = model.predict([encoder_input_data, decoder_input_data]).argmax(axis=2)\n","            decoder_input_data[:,i] = output[:,i]        \n","        return decoder_input_data[:,1:]\n","    \n","    def load(self, model_path='seq2seq_attn.h5'):\n","        return load_model(model_path)\n","    \n","    def _get_callbacks(self, model_path='seq2seq_attn.h5'):\n","        es = EarlyStopping(monitor='loss', patience=9, mode='auto', verbose=1)\n","        save_best = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only=True, save_weights_only=False, period=2)\n","        st = Sampletest()\n","        # hd = HistoryDisplay()\n","        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n","        return [rlr, es, st]#st, save_best,  hd,"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DzGG3GhaxHZ-","colab_type":"text"},"cell_type":"markdown","source":["## Compile model "]},{"metadata":{"id":"sz5V330IxHZ_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"KKjxhF2ZxHaF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"F6godnwCSV0r","colab_type":"text"},"cell_type":"markdown","source":["Comprobar si los shapes son iguales en keras que en numpy!"]},{"metadata":{"id":"nRtE8bcFxHaL","colab_type":"text"},"cell_type":"markdown","source":["## Train"]},{"metadata":{"id":"AOcX4SMBxHaO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# data_train"],"execution_count":0,"outputs":[]},{"metadata":{"id":"coFazOl3xHaR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"O7h2z9smxHaX","colab_type":"text"},"cell_type":"markdown","source":["## Predict"]},{"metadata":{"id":"JhB1Kxsgx1lg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}